\documentclass[11pt,english]{article}

\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm, columnsep=0.6cm, textheight=24.7cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{nonfloat}
\usepackage{url}
\usepackage{indentfirst}
% Package used for \author{} command. Feel free to remove
% if it doesn't suits your needs.
\usepackage[affil-it]{authblk}
%\usepackage{hyperref}
\usepackage[numbers, square, sort]{natbib}
\usepackage{subfig}
\usepackage{fixltx2e}
\usepackage{textcomp}
\usepackage{float}
\usepackage{floatflt}

\pagestyle{empty}

\newcommand{\engl}[1]{(engl.~\emph{#1})}

\setlength{\topskip}{0cm}

% List spaceing.
\renewcommand{\labelitemi}{\textendash}
\renewcommand{\labelitemii}{\textbullet}
\renewenvironment{itemize}{%
\begin{list}{\labelitemi}{%
\setlength{\topsep}{0mm}
\setlength{\itemsep}{-1mm}
\setlength{\labelindent}{\parindent}
\setlength{\leftmargin}{6mm}}}
{\end{list}}

% Setting right title format.
\let\LaTeXtitle\title
\renewcommand{\title}[1]{\LaTeXtitle{\Large \textbf{#1}}}
\renewenvironment{abstract}
{\noindent \large \bf Abstract. \normalsize \begin{it}}
{\end{it}\\}

% Adding dots after {sub}secions.
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\large\bfseries}{\thesubsubsection.}{1em}{}

\newenvironment{keywords}
{\noindent {\large {\bf Keywords}}.~}{}

% Space between authors-(authblk package)
\renewcommand\Authsep{ \quad }
\renewcommand\Authand{ \quad }
\renewcommand\Authands{ \quad }

% Space between affils.
\setlength{\affilsep}{0em}

% Bibliography
\renewcommand{\bibsection}{\section{References}}
\setlength{\bibsep}{2mm}

% Environment for tables inside of multicol
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}
\makeatother


\begin{document}

\title{Automatic authorship attribution for Croatian texts}
\author{Igor Belša}
\author{Tomislav Reicher}
\author{Ivan Krišto}
\author{Artur Šilić}
\affil{Faculty of Electrical and Computing Engineering, University of
Zagreb\\Unska 3, 10000 Zagreb, Croatia}
\affil{\{igor.belsa, tomislav.reicher, ivan.kristo, artur.silic\}@fer.hr}

% Removal of date. Don't change.
\date{}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}
\begin{multicols}{2}

% TODO:
% - Još malo popraviti predložak

\begin{abstract}
Automatic authorship attribution is useful in automatic text classification
which becomes quite complex task if it's applied in large data sets (such as
application at web search engines). Most of reported methods are language
specific, therefore we have developed methods specific for Croatian language
which range from simple, based on stylistic measures and function words, to
complex which require syntactic analysis of Croatian language. We have found that
various combinations of methods are very successful in solving the authorship
attribution problem.
\end{abstract}

% Please, insert keywords:
\begin{keywords}
Authorship attribution, SVM, function words, Croatian language syntactic
analysis.
\end{keywords}

\section{Introduction}
Automatic authorship attribution can be interpreted as a problem of text
classification based on its lingvistic features. Problems similar to authorship
attribution are detection of age, region and gender of the author
\citep{luyckx2005shallow}. Main concern of computer--assisted authorship
attribution is to define an appropriate characterization of document that
captures the writing style of authors \citep{coyotl2006authorship}.

% \citet{kukushkina2001using} notes that first works on authorship attribution
% problem are ``\emph{Izv.\ otd.\ russkogo jazyka i slovesnosti, Imp.\ akad.\ nauk.}'',
% Morozov, N., A.\ from 1915.\  and ``\emph{On some application of statistical
% method}'' from 1916.\ by Markov, A., A., founder of Theory of Markov chains.

Authorship attribution can help in document indexing, document filtering and the
hierarchical categorization of web pages and web search engines
\citep{luyckx2005shallow}. It is important to note that authorship attribution
is different from plagiarism detection. Plagiarism detection attempts to detect the
similarity between two substantially different pieces of work but is unable to
determine if they were produced by the same author \citep{de2001mining}.

The problem can be divided in three categories \citep{zhao2005effective}: binary,
multi--class and single--class (or one--class) classification. Binary
classification solves the problem when each document from data set is written by
one of two authors. Multi--class classification is generalization of binary
classification in which we have more than two authors. Single--class
classification is applied when some of the documents from the data set are written by
a particular author while the authorship of the other documents is unspecified.
This classification answers the question does a given document belong to
a single known author or not.

This paper presents a study of the problem of multi--class classification for
documents in Croatian language, oriented on evaluation of different sets of
documents features.

The rest of the paper is organized as follows. Section 2 discusses some previous
works on authorship attribution and similar problems. Section 3 describes used
data set. Section 4 is the most significant one. It introduces used methods.
Section 5 describes classification and section 6 discusses evaluation and
gives evaluation results. Conclusion and future work are given in section 7.

\section{Related work}
\citet{coyotl2006authorship} cluster methods of authorship attribution in three
main approaches: \emph{stylistic measures}, \emph{syntactic cues} and
\emph{word--based} document features.

\emph{Stylistic measures} based document features is characterized by features
that take into account length of words and sentences and the
richness of the vocabulary. \citet{coyotl2006authorship} notes that such features are not
sufficient to resolve problems that depend on the genre of the text and that they
lose their meaning when applied over short texts.

When using the \emph{syntactic cues as document features} -- informations
related to structure of the language which are obtained by an in--depth syntactic
analysis of documents, text is characterized by the presence and frequency of
certain syntactic structures. This characterization is detailed and relevant.
Unfortunately, it is computationally expensive and even impossible to build for
languages lacking the text--processing resources (e.g.\ POS tagger, syntactic
parser, etc.). It is clearly influenced by the length of documents.

Approach which uses \emph{Word--based} document features branches in three
different methods. \emph{The first one} characterizes documents using a set of
functional words (their presence and frequency), ignoring the content words since
they tend to be highly correlated with the document topics. \emph{Second branch}
of methods observes a document as a bag--of--words and uses single content words as
document features. This method can be applied only when there is a noticeable
correlation between the authors and the topics. The \emph{third branch} considers
word \emph{n}--grams as features, i.e.\ features consisting of sequences of
consecutive words.
% Nevertheless, due to the feature explosion, it tends to use
% only \emph{n}--grams up to three words.

\citet{kukushkina2001using} explain a method which uses algorithms for data
compression to identify the author. The appendix of this paper shows evaluation results
for authorship attribution with different compression algorithms. The idea behind
the method is to divide documents by authors, compress every set with selected
algorithm and write the size of the archive. To classify text of an unknown author,
text is added to each set and than the same data compression algorithm is applied. The author of
the documents in archive which records the smallest increase of the size is declared
as the author of the new document. Review of a similar method can be found in the
paper \citep{zhao2005effective}, which notes that the method has obvious
omissions and cites one paper that proves it.

\citet{koppel2003exploiting} show the use of gramatical errors and informal
styling (e.g.\ writing sentences in capital letters) as document features in
order to identify the author. Method is applicable only to unedited texts (blogs,
Internet forums, newsgroups, e--mail messages, etc.).

\citet{peng2003language} suggest the use of \emph{n}--gram language model to identify
the author. A similar method is shown in \citep{coyotl2006authorship}.

The success of functional words over collocations (certain pair of words
occurring within a given threshold distance of each other) is shown in
\citep{argamon2005measuring} -- however, because of information reduction when 
using function words as features, one might conclude the opposite -- which would be
wrong. \citet{argamon2005measuring} believe that most of the discriminating power
of collocations is due to the frequent words they contain (and not the
collocations themselves). They also note that using more training texts than
features seriously reduces the likelihood of overfitting the model to the
training data, improving the reliability of results. Similar claim can be found
in \citep{banko2001scaling} which deals with influence of the corpus size in
general natural language processing.

A similar comparison of the feature types is shown in
\citep{uzuner2005comparative} where the function words and syntactic
elements are compared in order to identify the author of document. It is
concluded that the syntactic elements of expression are useful as functional words in solving the
problem.

Description of the authorship attribution process by using syntactic elements
of language can be found in the extremely influential work
\citep{stamatatos2001computer}. Various types of style markers can be found in
\citep{luyckx2005shallow}.

\citet{stamatatos2001computer} suggest vocabulary diversity of the authors as
feature for identifying the author. It is measured by the ratio of unique words
and the total size of the text or counting words which occur only once
(\emph{hapax legomena}), words which occur only twice (\emph{dis legomena}), etc.
These measures are closely related to length of the text. They also note that
functions defined by Yule (1944) and Honore (1979) should solve this problem,
i.e.\ they should be constant regarding the length of the text. Analysis of
similar function can be found in the \citep{tweedie1998variable} which claims
that most of these functions are not independent regarding the length of the
text. It is concluded that the diversity of vocabulary is unstable feature for
texts shorter than 1000 words.

The problem of dependency on the length of the text for the classification using
a SVM (Support Vector Machine) is explained in \citep{diederich2003authorship}.
To avoid the dependency, a combination of $L_p$ normalization of the
length and transformation of terms occurrence frequency such as \emph{idf}
(inverse document frequency) measure was used.

Idf measure is defined as
\begin{equation}
F_{idf}(t_k) = \log \frac{n_d}{n_d(t_k)},
\label{equ:idf}
\end{equation}
where $n_d(t_k)$ is number of documents which contain term $t_k$ and $n_d$
total number of documents. Shown measure gives high values for terms which
appear in a small number of documents and thus it is very discriminatory.

\section{Data set}
\label{sec:podatci}
The online archive of ``Jutarnji list'' newspaper columns available at
\url{http://www.jutarnji.hr/komentari/} was used as the data set. The data set
consists of 4571 texts written by 25 authors. The lowest average number of words
in document per author is 315 words, and the highest is 1347 words. An
average number of words in document per author for this data set is 717 words. Number
of documents per author in the set is shown on figure
\ref{fig:articlesPerAuthor}.

To avoid the learning of topic, the data set is split by dates
-- 20\% of newest texts of every author is taken for testing (hold--out method).

\begin{minipage}{0.8\linewidth}
\vspace{10pt}
\centerline{\resizebox{1.4\linewidth}{!}{\input{figures/articlesPerAuthor}}}%
\figcaption{\small \textbf{\textsf{Number of texts per author.}}}%
\label{fig:articlesPerAuthor}
\end{minipage}

\section{Document features}
The main problem with the construction of an authorship attribution system is the
selection of the sufficiently discriminatory features of the text. A feature is
discriminatory if it is common for one author, and rare for all the others. Due to the
large number of authors, complex features are extremely useful if their
distribution is specific for each author (e.g.\ the frequency distribution
of some words).

Features can be combined. If complex features are combined, new features can
represent more distributions and have greater discriminatory power.

During the creation of the feature vector, it is neccessary to make the
feature independent of the length or of the content of each document. That dependency reduces
generality of systems application and can lead to decreasment of accurancy
(e.g.\ relating author to concrete topic or terms).

Features can be expressed as vectors of real numbers which simplifies
classification. The set of documents is expressed as a set of vectors with equal dimensions.
Combining of the features is implemented by appending the apropriate feature
vectors.

% Depending on selected feature model, text analysis and creation of feature
% vectors can vary from computationally very trivial operations to very complex
% algorithms (such as obtaining syntactic features by analyzing natural language).


\subsection{Function words frequency}
\label{sec:funkcijske-rijeci}
Function words are words that have little (or none) semantic content of their
own, such as adverbs, prepositions, conjunctions, or interjections. They usually
indicate a grammatical relationship or generic property
\citep{zhao2005effective}.

% Usefulness of function words occurrence frequency for authorship attribution lies
% in the fact that they are the indicators of writing style.
Some of the less known function words, such as prepositions \emph{onkraj},
\emph{namjesto} and \emph{zavrh} are rarely used and may very well suggest the
author. However, even the frequency of use of frequent functional words can be
very well used to distinguish the author. Due to high frequency of use of
function words and their roles in the grammar, the author usually has no
conscious control over their use in a particular text
\citep{argamon2005measuring}.

Functional words are topic--independent, authors automatically (without conscious
control) use functional words which indicate their own style.
%During the changing
%of the language of documents, it is also neccessary to update the set of
%functional words with which the system works.
It is difficult to predict whether the function words will give equally good
results for different languages. Despite the size of research on this
topic, due to various languages, the type and the size of the documents, it is difficult to
conclude if these methods are generally effective \citep{zhao2005effective}.

In addition to function words, auxiliary verbs and pronouns are being
considered. Their frequencies might be representative for different
author styles.

Building the document feature vector is done by counting the appearances
of functional words, auxiliary verbs and pronouns in the document. The results
are written as vectors where each component corresponds to the number of occurences of
related words divided by the total number of the words in the document. This is done to remove
the dependency on the length of the document. For the function words that have not
appeared in a document, the resulting component always equals zero.

\subsection{Lexical categories frequency}
\label{sec:rijeci-grupe}
Building of the feature vector is done by counting the number of appearances of
different lexical categories where the categories considered are adverbs,
adpositions, conjunctions, particles, interjections, nouns, verbs, adjectives and
pronouns which are obtained by syntactic analysis of Croatian language. Result is
written as a vector (nine--dimensional vector) and each component of the vector
is divided by the number of words in the document.

\subsection{Idf weighted function words frequency}
\label{sec:funkcijske-rijeci-idf}
The feature vector is built by multiplying components of vector created by method
defined in \ref{sec:funkcijske-rijeci} and its associated \emph{idf}
\engl{inverse document frequency} weight. \emph{Idf} weight is defined by
expression (\ref{equ:idf}) \citep{diederich2003authorship}.

Shown measure discriminate documents that contain functional words that are used
in small number of other documents. The disadvantage of \emph{idf} measure is
that it records only presence of certain term in the document, term
counting in the document is ignored. Therefore, word which appears many times in
one text and one time in the others gets the same value as the one which appears
once in all text---the word which would very well separate one document from the
others is ignored.

\subsection{Punctation marks, vowels, words length and sentence length
frequency}
\label{sec:znacajke-manje}
A set of following punctuation marks are used: ``.'', ``,'', ``!'', ``?'',
``''', ``"'', ``-'', ``:'', ``;'', ``+'', ``*'' and their number of appearance in
document is counted. Result is written as 11--dimensional vector and every
component is divided by total sum of characters in the document.

Feature vector based on the frequency of vowels occurence (a, e, i, o, u) is
obtained in equal procedure as for the punctuation marks

Frequency of word lengths are obtained by counting the words that have equal
length. It is important to note that this procedure can lead to vectors of
different features dimensions (e.g.\ a document has a word with length of 11,
but some other document doesn't). The issue is solved by limiting the maximum
length of words at length of 10. All words longer than 10 characters are
counted to the 10th group. It is necessary to divide components of resulting feature
vector with number of words in document (to remove dependency on text length).

The sentence length frequency is obtained in equal procedure as for the word
length frequency. An different vectors dimension issue is solved by limiting
sentence length to 20.

Suggested features have weak discriminatory power on their own, but they are
very useful in combination with other features (see \ref{sec:evaluacija}).

\subsection{Word type \emph{n}--grams frequency}
\label{sec:ngrami-tipova}
Features are based on word type \emph{n}--grams frequency. Word types and their
morphosyntactic descriptors are obtained by POS (Part--Of--Speech) tagging for
Croatian language. Using \emph{n}--grams as features can produce very large
number of dimensions, therefore only 3--grams are used.

Method branches in two feature types. First type uses pure word types (nouns,
verbs, adjectives, pronouns, conjunctions, interjections and prepositions), for
example word 3--gram ``Adam i Eva'' transforms to ``noun conjunction noun.''
Second type transforms nouns, verbs and adjectives to their types, and for other
word types uses words as they are, therefore ``Adam i Eva'' transforms to
``noun i noun.'' Due to many different pronouns, conjunctions, interjections and
prepositions frequency filtering is applied---frequency is counted for only 500
most frequent 3--grams in training data set. Used dimension reduction method is
not optimal, therefore in future work other methods should be evaluated, such as
information gain, $\chi^2$ test, mutual information, maximum relevance, minimum
redundancy or classification with sparse SVM, logistic regression or na\"ive
Bayes.

\subsection{Word morphosyntactic elements}
\label{sec:morphosyntactic}
Building of feature vector is done by counting appearances of morphosyntactic
elements for every word in document and dividing them by number of words in text.
Counted morphosyntactic elements are \emph{case}, \emph{degree}, \emph{form},
\emph{gender}, \emph{number} and \emph{person}.

\section{Classification}
% Representation of documents by real number vectors  enables easy use of
% classifiers that search decision functions, i.e.\ boundaries in vector
% space.

The used classifier is SVM (Support Vector Machine) with radial basis
function as kernel. It is shown that, with selection of right parameters, linear
SVM is special case of SVM with RBF kernel \citep{keerthi2003asymptotic} which
removes need to use linear SVM as potential classifier.

It is required to scale data for use with SVM to ensure equal contribution of
every attribute to classification. We scale components of every feature vector
to interval of $[0, 1]$.
%  according to following expression:
% \begin{equation}
% x^{s}_{i,j} = \frac{x_{i,j} - \min_{i}\; x_{i,j}}{\max_{i}\; x_{i,j}
% - \min_{i}\; x_{i,j}}
% \end{equation}
% where $x^{s}_{i,j}$ is scaled component $j$ of vector $\mathbf{x_i}$,
% $\min_{i}\; x_{i,j}$ is minimum value of attribute $j$ among all vectors
% $\mathbf{x_i}$ and $\max_{i}\; x_{i,j}$ is maximum value of attribute $j$ among
% all vectors. If we denote the resulting minimum and maximum values as follows:
% \begin{eqnarray}
% M_i & = \max_{i}\; x_{i,j} \\
% m_i & = \min_{i}\; x_{i,j}
% \end{eqnarray}
% then, the unknown vector $\mathbf{x}$ before classification is scaled as:
% \begin{equation}
% x^{s}_{j} = \frac{x_j-m_i}{M_i-m_i}
% \end{equation}

Parameter search is done by model selection with parameters $(C, \gamma)$ from
set $\left (C = {2^{-5}, 2^{-4}, \ldots , 2^{15}},  \gamma = {2^{-15}, 2^{-14},
\ldots, 2^3} \right )$ \citep{CC01a} which give the highest accuracy of
classification in process of cross--validation on learning data set. Accuracy
of classification is measured by expression:
\begin{equation}
acc = \frac{n_c}{N}, % ako bude problema, ovo staviti kao ``inline'' jednadžbu.
\end{equation}
where $n_c$ is number of correctly classified documents, $N$ is total number of
documents. After we find parameters $(C, \gamma)$ for which system has highest
accuracy, we learn classifier with given parameters.


\section{Evaluation}
\label{sec:evaluacija}
Data set is split on set for learning and set for testing by taking 20\%
documents of each of 25 authors for testing. Data set for testing consists of
1146 documents. Classification success is measured by ratio of correctly
classified documents and total number of documents (accuracy).

Results of evalutaion are shown in table \ref{tbl:eval}. For shorter record,
method defined in \ref{sec:funkcijske-rijeci} is marked as ``$\mathcal{F}$,''
method defined in \ref{sec:rijeci-grupe} is marked as ``$\mathcal{C}$,'' method
from \ref{sec:funkcijske-rijeci-idf} ``$\mathcal{I}$,'' methods from
\ref{sec:znacajke-manje} respectivly ``$\mathcal{P}$,'' ``$\mathcal{V}$,'' 
``$\mathcal{L}$'' and ``$\mathcal{S}$,'' methods from \ref{sec:ngrami-tipova}
``$\mathcal{N}_1$'' and ``$\mathcal{N}_2$'' and method from
\ref{sec:morphosyntactic} ``$\mathcal{M}$.'' Column ``Correct'' denotes
number of correctly recognized authors, ``Incorrect'' -- incorrectly recognized.
Columns ``$C$'' and ``$\gamma$'' denote parameters of SVM classifier used for
selected model.

Highest accuracy during cross--validation is achived by combination of
methods marked as $\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$,
$\mathcal{L}$ and $\mathcal{M}$.

%% TODO: Što s ovim?
% Total accuracy doesn't explains behaviour of classifier for every class by
% itself. For every class precision and recall are calculated and by them we
% calculate total weighted $F$ measure. For particular class $c$, precision is
% ratio of number of correctly classified documents in $c$ with number of all
% documents which are classified as $c$. Recall is ratio of number of correctly
% classified documents in $c$ with number of all documents in $c$. $F$ measure is
% calculated for every class $c_i$ according to following expression:
% \begin{equation}
% F_i = \frac{2 \cdot precision_i \cdot recall_i}{precision_i + recall_i}
% \end{equation}
% where $precision_i$ and $recall_i$ are measures of precision and recall for
% class $c_i$.
% 
% Total weighted measure is calculated by following expression:
% \begin{equation}
% F_u = \frac{\sum^{n}_i |c_i|\cdot F_i}{\sum^n_i|c_i|}
% \end{equation}
% where $n$ is total number of classes, $|c_i|$ number of documents in class
% $c_i$ and $F_i$ a $F$ measure for class $c_i$.
% 
% Weighted $F$ measure at our data set for testing gives value of 87\%.

\begin{table*}
\begin{center}
\caption{\small \textbf{\textsf{Evaluation of different features}}}%
\begin{tabular}{l c c c c c}
\toprule
Method & Correct & Incorrect & Accuracy & $C$ & $\gamma$ \\
\midrule
$\mathcal{F}$ & 1013 & 133 & 88.39\% & 8192 & 0.125\\
$\mathcal{I}$ & 1008 & 138 & 87.96\% & 8192 & 0.125\\
$\mathcal{C}$ & 510 & 636 & 44.50\% & 512 & 2.0\\
$\mathcal{P}$ & 659 & 487 & 57.50\% & 8192 & 0.125\\
$\mathcal{V}$ & 350 & 796 & 30.54\% & 128 & 0.125\\
$\mathcal{L}$ & 495 & 651 & 43.19\% & 128 & 0.125\\
$\mathcal{S}$ & 485 & 661 & 42.32\% & 128 & 0.125\\
$\mathcal{N}_1$ & 817 & 329 & 71.29\% & 512 & 0.125\\
$\mathcal{N}_2$ & 872 & 274 & 76.09\% & 512 & 0.125\\
$\mathcal{M}$ & 701 & 445 & 61.17\% & 512 & 0.125\\
$\mathcal{C}$, $\mathcal{M}$ & 724 & 422 & 63.17\% & 8192 & 0.03125\\
$\mathcal{P}$, $\mathcal{F}$ & 1051 & 95 & 91.71\% & 8 & 0.03125\\
$\mathcal{F}$, $\mathcal{M}$ & 1045 & 101 & 91.18\% & 128 & 0.03125\\
$\mathcal{F}$, $\mathcal{N}_1$ & 1025 & 121 & 89.44\% & 128 & 0.03125\\
$\mathcal{F}$, $\mathcal{N}_2$ & 1014 & 132 & 88.48\% & 128 & 0.03125\\
$\mathcal{I}$, $\mathcal{M}$ & 1041 & 105 & 90.84\% & 128 & 0.03125\\
$\mathcal{N}_1$, $\mathcal{M}$ & 818 & 328 & 71.38\% & 128 & 0.03125\\
$\mathcal{I}$, $\mathcal{M}$, $\mathcal{C}$ & 1047 & 99 & 91.36\% & 128 & 0.03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$ & 1067 & 79 & 93.11\% & 128 & 0.03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$, $\mathcal{M}$ & 1071 & 75 & 93.46\% & 32768 & 0.03125\\
$\mathcal{F}$, $\mathcal{M}$, $\mathcal{C}$, $\mathcal{N}_1$ & 1027 & 119 & 89.62\% & 128 & 0.03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$ & 1068 & 78 & 93.19\% & 128 & 0.03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$, $\mathcal{N}_1$ & 1059 & 87 & 92.41\% & 128 & 0.03125\\
\bottomrule
\end{tabular}
\label{tbl:eval}%
\end{center}
\end{table*}%

\section{Conclusion}
It is shown that authorship attribution problem can be successfully solved by
the relatively simple methods, although for state--of--art methods, we
believe that syntactic analysis of documents is required. Our results with
93\% accuracy fit very well in interval of reported results which range from
70\% to 97\% \citep{coyotl2006authorship,keselj2003n,luyckx2005shallow,stamatatos2001computer}.

Result comparision is difficult due to different types of data sets (e.g.\ poems,
newspaper articles, e--mails) and the problems (binary, multi--class or
single--class classifications). There are no relevant data sets for comparision
\citep{zhao2005effective}, but there are references on paper
\citep{stamatatos2001computer} and their ``Greek data set'' (e.g.\
\citep{keselj2003n}). There is an significant affected of data set type (number
and variety of document samples) on the complexity of the problem.

In future work, methods based on word and character \emph{n}--grams suggested
in \citep{keselj2003n,peng2003language,coyotl2006authorship} has to be
evaluated. Also, evaluation has to be performed on different types of data sets
such as poems, newspaper articles or books data sets.

\bibliographystyle{plainnat}
\bibliography{literatura}

\end{multicols}

\end{document}
