\documentclass{llncs}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{url}
\usepackage{lmodern}
\usepackage{nonfloat}

\begin{document}

\title{Automatic Authorship Attribution for Texts in\\Croatian Language Using
Combination of Features}
\author{Tomislav Reicher \and Ivan Krišto \and Igor Belša \and Artur Šilić}
\institute{Faculty of Electrical and Computing Engineering\\
University of Zagreb\\
Unska 3, 10000 Zagreb, Croatia\\
\email{\{tomislav.reicher, ivan.kristo, igor.belsa, artur.silic\}@fer.hr}}

\maketitle

\begin{abstract}
In this work we investigate the use of various character, lexical and syntactical
level features and their combinations in automatic authorship attribution. Since
the majority of text representation features are language
specific, as reported in the related work, we examine their application on the texts written in 
the Croatian language. The used data set is quite heterogeneous and consists of articles (journals)
from a daily Croatian newspaper written by 25 different authors.
For the classification we use powerful machine learning algorithm Support
Vector Machine to achieve excellent results of 93\% accuracy.

\vspace{10pt}
\textbf{Key words:} text classification, SVM, function words, MSD tagging.
\end{abstract}


\section{Introduction}
Automatic authorship attribution is a process in the field of text classification
that deals with author identification of a given text. It can
be interpreted as a problem of text classification based on linguistic features
specific to certain authors. Problems similar to authorship attribution are
detection of author's age, region or gender \cite{luyckx2005shallow}. The main
concern in computer-based authorship attribution is defining an appropriate
characterization of the text. Such characterization should capture the writing
style of the authors \cite{coyotl2006authorship}.

Authorship attribution can help in document indexing, document filtering and
hierarchical categorization of web pages \cite{luyckx2005shallow}. These
applications are common in the field of information retrieval. 
It must be noted that authorship attribution differs from plagiarism
detection. Plagiarism detection attempts to detect similarities between two substantially different pieces of
work. However, it is unable to determine if they were produced by the same author or not
\cite{de2001mining}.

The problem of authorship attribution can be divided into three categories
\cite{zhao2005effective}: binary, multi-class and single-class (or one-class)
classification. Binary classification solves the problem when the data set
contains the texts written by one of two authors. Multi-class classification is a
generalization of the binary classification when there are more than two authors
in the data set. One-class classification is applied when only some of the texts
from the data set are written by a particular author while the authorship of all
the other texts is unspecified. This classification ascertains whether a given
text belongs to a single known author or not.

This paper presents a study of multi-class classification for the texts written
in the Croatian language. The work is oriented on the combination and evaluation
of different text representations features. The rest of the paper is organized as
follows. Section 2 discusses related work in authorship attribution and similar
problems. Section 3 introduces different types of text representation features we
have utilized. Section 4 describes the classification, Section 5 describes the
used data set and Section 6 presents evaluation methods and experiment results.
The conclusion and future work are given in Section 7.

\section{Related Work}
There are several approaches to author attribution in respect of different text
representation features used for the classification. Based on those features, the
following taxonomy can be made \cite{stamatatos2009survey}: \emph{character features},
\emph{lexical features}, \emph{syntactic features}, \emph{semantic features} and
\emph{application--specific features}.  This taxonomy is mainly focused on computational requirements. 
The following paragraphs describe character, lexical and syncatic features 
in more depth and relate our work with the existing research.

\emph{Character features} are the simplest text representation features. They
consider text as a mere sequence of characters and are therby usable for any natural
language or corpus. Various meassures can be defined, such as
characters frequencies, digit frequencies, uppercase and lowercase character
frequencies, punctuation marks frequencies, etc.\ \cite{de2001mining}. Another
type of character based features, which has been proven as quite successful
\cite{peng2003language,stamatatos2006ensemble}, considers extracting
frequencies of character \emph{n}--grams. 

Text representation using \emph{lexical features} is characterized by dividing
the text into a sequence of tokens (words) that group into sentences. Features
directly derived from that representation are the length of words, the
length of sentences and vocabulary richness. This types of features have been used in
\cite{mendenhall1887,holmes1994authorship} . Results achived demonstrate that
they are not sufficient for the task mostly due to their
significant dependence on the text genre and length. However, taking advantage of
features based on frequencies of different words, especially function words,
can produce fairly better results
\cite{argamon2005measuring,uzuner2005comparative,koppel2003exploiting,zhao2005effective}.
Analogusly to character \emph{n}--grams, word \emph{n}--gram features
can be defined for which is shown to be quite successfull too
\cite{keselj2003n,coyotl2006authorship}.

The use of \emph{syntactic features} is governed by the idea that authors tend to
unconsciously use similar syntactic patterns. Information related to the structure of
the language is obtained by an in--depth syntactic analysis of the text, usually
using some sort of an NLP tool. A single text is characterized by the presence
and frequency of certain syntactic structures. Syntax-based features were
introduced in \cite{van1996outside}, where the rewrite rules frequencies were
utilized. Stamatatos et al.\ \cite{stamatatos2001computer} used noun, verb and
prepositional phrase frequencies. Using a Part--of--speech (POS) tagger one can obtain POS tags and
POS tag \emph{n}--gram frequencies. Excellent results can be achived using such features
\cite{kukushkina2001using,koppel2003exploiting,diederich2003authorship,luyckx2005shallow}.
Koppel et al.\ \cite{koppel2003exploiting} show the use of grammatical errors
and informal styling (e.g., writing sentences in capital letters) as text features used for
authorship attribution. A comparison of lexical and syntactical features is shown
in \cite{uzuner2005comparative}. 

Our work is based on the composition and evaluation of various afore--mentioned
text representation features. We use different character, lexical and syntactical
features and adapt them for the use with the Croatian language. We use punctuation marks and 
wovels frequency as character features. Word length, sentence length and function words frequencies are used as lexical features.
For the syntax--based features we use those relatively similar to POS tag and POS
tag \emph{n}--grams frequencies.


\section{Text Representation}
When constructing an authorship attribution system, the central issue is the
selection of sufficiently discriminative features. A feature is discriminative if
it is common for one author and rare for all the others. Due to the large number
of authors some complex features are very useful if their distribution is
specific to each author. Moreover, as the texts from dataset greatly differ in
length and topic, it is necessary to use the features independent of such
variations. If the features were not independent of such variation that
would most certainly reduce generality of system's application and could lead
to a decrease of accuracy (e.g., relating author to concrete topic or terms).

In the following subsections we will describe different features used.

\subsection{Function Words Frequencies}
\label{sec:funkcijske-rijeci}
Function words, such as adverbs, prepositions, conjunctions, or interjections,
are words that have little or no semantic content of their own. They usually
indicate a grammatical relationship or a generic property
\cite{zhao2005effective}. Although one would assume that frequencies of some of
the less used function words would be useful indicator of authors style even the
frequencies of more common function words can adequately distinguish between the
authors. Due to the high frequency of the function words and their significant
roles in the grammar, the author usually has no conscious control over their
usage in a particular text \cite{argamon2005measuring}. They are also
topic--independent. Therefore, function words are good indicators of the author's
style.

It is difficult to predict whether these words will give equally good results for
different languages. Moreover, despite the abundance of research in this field,
due to various languages, types and sizes of the texts, it is currently
impossible to conclude if these features are generally effective
\cite{zhao2005effective}.

In addition to function words in this work we also consider frequencies of
auxiliary verbs and pronouns as their frequencies might also be
representative of the style of different authors. This makes the set of totally
652 function words used.

\subsection{Idf Weighted Function Words Frequency}
\label{sec:funkcijske-rijeci-idf}

The use of features based on function words frequencies often implies the
problem with the quantifying how important, for the sake of discrimination, a
specific given function word is \cite{diederich2003authorship}.

To cope with that problem we used a combination of $L_p$ normalization of the
length and transformation of the function word occurrence frequency, in
particular \emph{idf} (inverse document frequency) measure.

Idf measure is defined as \cite{diederich2003authorship}
\begin{equation}
F_{idf}(t_k) = \log \frac{n_d}{n_d(t_k)},
\label{equ:idf}
\end{equation}
where $n_d(t_k)$ is the number of texts from learning data set that contain word
$t_k$ and $n_d$ the total number of the texts in that learning data set. The
shown measure gives high values for words that appear in a small number of
texts and are thus very discriminatory.

As the \emph{idf} measure uses only the information of the presence of a
certain function word in the texts ignoring frequency of that word, a word that appears
many times in one single text and once in all the others gets the same value as
the one which appears once in all of the texts. Therefore it is necessary to
multiply the obtained \emph{idf} measure, of the given function word, with the
occurrence frequency of that word in the observed text.

\subsection{Lexical Categories Frequency}
\label{sec:rijeci-grupe}
Next three features we used are syntax--based features and thus some sort of NLP
(Natural Language Processing) tool was required. Croatian language is
morphologically complex and it is problematic to construct a robust NLP tool.
With the tool we used, given in \cite{snajder08automatic}, we were able to get
morphosyntactic descriptions of each word in the text. The method does not
use context information and therefore cannot distinguish between different
homographs --- words with same spelling but with different meaning and probably
different POS too --- so all possible POS tags for a given word are considered.

Simplest syntax--based feature we used are features based on lexical category
frequency, similar to one used in \cite{kukushkina2001using}. The given text is
tagged with the use of NLP tool and thus lexical category for each word in the text is determined. Features are obtained by
counting the number of occurrence of different lexical categories, which are
then normalized by the total number of words in the text. If for some word more
then one lexical category is obtained, all different categories where counted. 
The used categories are adverbs, appositions, conjunctions, particles,
interjections, nouns, verbs, adjectives and pronouns. In addition, category
``unknown'' was introduced for all the words whose lexical category was not
determined (names, places, etc.).

\subsection{Word Morphosyntactical Categories}
\label{sec:morphosyntactic}

More complex syntax--based features we used take advantage of morphosyntactic
description of a word. Each word can be described by the set of different
morphosyntactic as are features that take The feature vector is created by
counting the appearances of morphologic categories for every word in a text, and dividing them by the number of words in
the text. Counted morphologic categories are \emph{case}, \emph{degree},
\emph{form}, \emph{gender}, \emph{number} and \emph{person}.

\subsection{Word Part--Of--Speech \emph{n}-grams Frequency}
\label{sec:ngrami-tipova}
The two proposed features are based on word part--of--speech \emph{n}-grams
frequency. Word parts--of--speech and their morphosyntactic descriptors are obtained by POS
(Part--Of--Speech) and MSD (morphosyntactic) tagging for Croatian language
\cite{snajder08automatic}. Having the corresponding part--of--speech for
each word in the text makes the usage \emph{n}-grams as features
possible. As \emph{n}-gram features can produce very large dimensionality, 
only 3-grams are considered. In addition, POS tagging used is not perfect. 

%FIXME: pos patterns, not n-grams
The first feature proposed uses the words parts--of--speech
(nouns, verbs, adjectives, pronouns, conjunctions, interjections and
prepositions) to form various \emph{n}-grams and count their frequencies. For
example, word 3-gram ``Adam i Eva'' (``Adam and Eve'') forms ``noun
conjunction noun'' trigram. The second feature proposed uses only words
parts--of--speech information for nouns, verbs and adjectives and for other word
parts--of--speech it uses words as they are, therefore ``Adam i Eva'' transforms
to ``noun i noun''. Due to many different pronouns, conjunctions, interjections
and prepositions that make many different \emph{n}-grams, frequency filtering
is applied --- only the frequency of 500 most frequent 3-grams in the
training data set is considered. Used dimension reduction method is not optimal,
therefore in the future work other methods should be evaluated, such as information
gain, $\chi^2$ test, mutual information, maximum relevance, minimum redundancy
or classification with sparse SVM, logistic regression or na\"ive Bayes.

\subsection{Other Features}
\label{sec:znacajke-manje}
Other features we used are simple features: punctuation marks, vowels, words
length and sentence length frequencies.

A set of following punctuation marks is used: ``.'', ``,'', ``!'', ``?'',
``''', ``"'', ``-'', ``:'', ``;'', ``+'', ``*''. Their appearance in the text is counted and
the result is normalized by the total number of characters in the text. 

Features based on the frequency of vowel occurrence (a, e, i, o, u) are
obtained in an equal manner.

The frequencies of words lengths are obtained by counting the lengths of all
the words form a text and then normalizing them by the total number of words in
text. To enforce consistency, the words longer than 10 characters are counted as
if they were 10 characters long.

The sentence length frequency is obtained in a similar procedure.
For the same reason as with the words length, sentences longer than 20 words are
counted as if they were 20 words long.

All features suggested here int this subsection have weak discriminatory power
on their own. However, they proved very useful in the combination with other features, as
shown in Table \ref{tbl:eval}.

\section{Classification}
% Representation of documents by real number vectors  enables easy use of
% classifiers that search decision functions, i.e., boundaries in vector
% space.
All the features mentioned in this work use some sort of frequency information
that makes it possible to represent them by real valued feature vectors. Having
features in a form of vectors, for the classification, we used an SVM (Support
Vector Machine) with radial basis function as the kernel. It is shown that, with
the use of parameter selection, linear SVM is a special case of an SVM with RBF
kernel \cite{keerthi2003asymptotic}, which removes the need to consider a linear
SVM as a potential classifier. RBF kernel is defined as:
\begin{equation}
k(\mathbf{x_i},\mathbf{x_j})=\exp(-\gamma \|\mathbf{x_i} - \mathbf{x_j}\|^2).
\end{equation}

Before the learning process and classification with the SVM, we scale the data,
real valued feature vectors, to ensure equal contribution of every attribute to
the classification. Components of every feature vector are scaled to an interval
$[0, 1]$.
%  according to following expression:
% \begin{equation}
% x^{s}_{i,j} = \frac{x_{i,j} - \min_{i}\; x_{i,j}}{\max_{i}\; x_{i,j}
% - \min_{i}\; x_{i,j}}
% \end{equation}
% where $x^{s}_{i,j}$ is scaled component $j$ of vector $\mathbf{x_i}$,
% $\min_{i}\; x_{i,j}$ is minimum value of attribute $j$ among all vectors
% $\mathbf{x_i}$ and $\max_{i}\; x_{i,j}$ is maximum value of attribute $j$ among
% all vectors. If we denote the resulting minimum and maximum values as follows:
% \begin{eqnarray}
% M_i & = \max_{i}\; x_{i,j} \\
% m_i & = \min_{i}\; x_{i,j}
% \end{eqnarray}
% then, the unknown vector $\mathbf{x}$ before classification is scaled as:
% \begin{equation}
% x^{s}_{j} = \frac{x_j-m_i}{M_i-m_i}
% \end{equation}

% TODO: REPHRASE

For the application of SVM classifier in practical problems it is reasonably to
consider SVM with soft margins defined by parameter $C$, as in 
\cite{cortes1995support}. Parameter $C$ together with $\gamma$ used in RBF
kernel completly define SVM model. The search of the appropriate parameters
$(C, \gamma)$, i.e., model selection, is done by the means of cross-validation:
using the 5--fold cross--validation on the learning set parameters $(C, \gamma)$ that give the highest accuracy are selected and the SVM classifier is learned using them. The accuracy of classification is measured by the expression:
\begin{equation}
acc = \frac{n_c}{N}, % ako bude problema, ovo staviti kao ``inline'' jednadžbu.
\end{equation}
where $n_c$ is the number of correctly classified texts and $N$ is the total number of
texts.
Parameters $(C, \gamma)$ that were considered are: $C \in \{2^{-5}, 2^{-4},
\cdots , 2^{15}\}$, $\gamma \in \{2^{-15}, 2^{-14}, \cdots, 2^3\}$ \cite{CC01a}.

\section{Data Set}
\label{sec:podatci}
We used an online archive of \emph{proofread} articles (journals) from a daily
Croatian newspaper ``Jutarnji list,'' available at
\url{http://www.jutarnji.hr/komentari/}. The data set consists of 4571 texts
written by 25 different authors. The texts are not evenly distributed
among authors. The number of texts per author in the used data set is shown in
Figure \ref{fig:articlesPerAuthor}. The articles are not of the same size --- the 
lowest average number of words in the text per author is 315 words, and
the highest average is 1347 words. An average number of words in a text per author is 717 words.
Considering this analysis, we can conclude that the used data set is very
heterogeneous.

Since the writing topics in these articles tend to be time--specific, to avoid the
overfitting, we split the set by dates --- 20\% of the newest articles of each author are
taken for testing (hold--out method). Therefore, the training set contains 3425 texts
and the testing set 1146 texts.

\begin{minipage}{0.8\linewidth}
\vspace{10pt}
\centerline{\resizebox{0.7\linewidth}{!}{\input{figures/articlesPerAuthor}}}%
\figcaption{Number of texts per author.}%
\label{fig:articlesPerAuthor}
\end{minipage}

\begin{table*}[htb]
\begin{center}
\caption{Evaluation of Different Features}%
\begin{tabular}{c c c r@{.}l c}%
\toprule%
Features & Accuracy [\%] & $C$ & \multicolumn{2}{c}{$\gamma$} & $F_w$ [\%]\\
\midrule
Function Words ($\mathcal{F}$) & 88.39 & 8192 & 0 & 125 & 87.38\\
Idf Weighted Function Words ($\mathcal{I}$) & 87.96 & 8192 & 0 & 125 & 86.84\\
Lexical Categories ($\mathcal{C}$) & 44.50 & 512 & 2 & 0 & 38.18\\
Punctation Marks ($\mathcal{P}$) & 57.50 & 8192 & 0 & 125 & 52.93\\
Vowels ($\mathcal{V}$) & 30.54 & 128 & 0 & 125 & 16.24\\
Words Length ($\mathcal{L}$) & 43.19 & 128 & 0 & 125 & 33.22\\
Sentence Length ($\mathcal{S}$) & 40.49 & 128 & 0 & 125 & 33.70\\
Word POS \emph{n}-grams -- 1st method ($\mathcal{N}_1$) & 71.29 &
512 & 0 & 125 & 68.68\\
Word POS \emph{n}-grams -- 2nd method ($\mathcal{N}_2$) & 76.09
& 512 & 0 & 125 & 72.52\\
Word Morphosyntactical Categories ($\mathcal{M}$) & 61.17 & 512 & 0 & 125 & 58.91\\
$\mathcal{C}$, $\mathcal{M}$ & 63.17 & 8192 & 0 & 03125 & 62.06\\
$\mathcal{P}$, $\mathcal{F}$ & 92.41 & 8 & 0 & 03125 & 91.93\\
$\mathcal{F}$, $\mathcal{M}$ & 91.18 & 128 & 0 & 03125 & 90.68\\
$\mathcal{F}$, $\mathcal{N}_1$ & 89.44 & 128 & 0 & 03125 & 88.52\\
$\mathcal{F}$, $\mathcal{N}_2$ & 90.92 & 128 & 0 & 03125 & 90.43\\
$\mathcal{I}$, $\mathcal{M}$ & 90.84 & 128 & 0 & 03125 & 90.35\\
$\mathcal{N}_1$, $\mathcal{M}$ & 71.38 & 128 & 0 & 03125 & 70.15\\
$\mathcal{I}$, $\mathcal{M}$, $\mathcal{C}$ & 91.36 & 128 & 0 & 03125 & 90.89\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$ & \textbf{93.37} & 128 & 0 & 03125
& 92.96\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$, $\mathcal{M}$ & \textbf{93.46} &
32768 & 0 & 03125 & 93.09\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$ & \textbf{93.37} &
128 & 0 & 03125 & 93.04\\
$\mathcal{F}$, $\mathcal{M}$, $\mathcal{C}$, $\mathcal{N}_1$ & 89.62 & 128 & 0
& 03125 & 88.70\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$, & 92.67 & 128 & 0 &
03125 & 92.18\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{N}_2$, $\mathcal{L}$ & 83.33 & 8 & 0 &
03125 & 81.68\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, &
93.19 & 128 & 0 & 03125 & 92.85\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$,
$\mathcal{M}$ & 93.19 & 128 & 0 & 03125 & 92.88\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$,
$\mathcal{L}$, $\mathcal{M}$, $\mathcal{N}_1$ & 92.41 & 128 & 0 & 03125 & 91.96\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$,
$\mathcal{M}$, $\mathcal{N}_1$, $\mathcal{C}$ & 92.41 & 128 & 0 & 03125 &
91.96\\
\bottomrule%
\end{tabular}%
\label{tbl:eval}%
\end{center}
\end{table*}

\section{Evaluation}
\label{sec:evaluacija}
Classification success is measured by the ratio of correctly classified texts and
the total number of texts in the training set i.e.,~micro average accuracy.
First we tested all the features separately and then the same features in
various combinations. The SVM parameter selection is very time
consuming process. Thus, not all of the feature combinations were tested. Results of the evaluation
are shown in Table \ref{tbl:eval}. Columns ``$C$'' and ``$\gamma$'' denote
parameters of SVM classifier optimal for the selected features.

% Highest accuracy during cross--validation is achived by combination of
% methods marked as $\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$,
% $\mathcal{L}$ and $\mathcal{M}$.

Total accuracy does not explain the behavior of a classifier for every class by
itself. Therefore, precision and recall are calculated for every class and then
used to calculate the total weighted $F_1$ measure. For particular class $c$,
precision is a ratio of the number of correctly classified documents in $c$ and the number
of all documents which are classified as $c$. Recall is a ratio of the number of correctly
classified documents in $c$ and the number of all the documents in $c$. $F_1$ measure
is calculated for every class $c_i$ according to the following expression:
\begin{equation}
F_i = \frac{2 \cdot precision_i \cdot recall_i}{precision_i + recall_i}
\end{equation}
where $precision_i$ and $recall_i$ are measures of precision and recall for
the class $c_i$.

Total weighted measure is calculated by the following expression:
\begin{equation}
F_w = \frac{\sum^{n}_i |c_i|\cdot F_i}{\sum^n_i|c_i|}
\end{equation}
where $n$ is total number of classes, $|c_i|$ number of documents in the class
$c_i$ and $F_i$ an $F_1$ measure for class $c_i$.

The evaluation was not conducted on all of the possible methods. We focused
on the evaluation of the methods based on syntactic analysis of the Croatian language.
Furthermore, some of the methods are very similar and the evaluation would show no
considerable improvements. %((N1 i N2, te F i Idf (I))
In addition, methods using certain feature combinations made no significant contributions.
Since no conclusions could be derived from them, they are not presented within the results. 
%navesti detaljnije koje za svaku tvrdnju (recimo u zagradi), ili ostaviti ovako?

\section{Conclusion}
It is shown that the authorship attribution problem, when applied to morphologically complex
languages, such as the Croatian language, can be successfully solved using the
combination of some relatively simple features. Our results with 93\% accuracy
are quite notable considering the fact that the data set used was very
heterogeneous, with the imbalanced distribution of texts over the authors, texts
of a variable but rather small size (an average of 717 words per author). The
result comparison has proved very difficult due to the different types of data sets used
in the related work (e.g., poems, newspaper articles, e-mails) and different
number of authors considered (e.g., 2, 5, 10, 25, 150). However, our results
coincide within the interval of previously reported results, which range from
70\% to 97\%
\cite{coyotl2006authorship,keselj2003n,luyckx2005shallow,stamatatos2001computer}.
In addition, there are no other reported methods or results for the Croatian
language, nor any of the related languages.

In the future work, methods based on word and character \emph{n}-grams, suggested
in \cite{keselj2003n,peng2003language,coyotl2006authorship}, should be
compared to our methods. Moreover, evaluation has to be performed on different
types of data sets, such as poems, newspaper articles or book data sets.


\section*{Acknowledgement}
This research has been supported by the Croatian Ministry of Science, Education and Sports under the grant No.036-1300646-1986.

\bibliographystyle{splncs}
\bibliography{literatura}

\end{document}