\documentclass[11pt,english]{article}

\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm, columnsep=0.6cm, textheight=24.7cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{nonfloat}
\usepackage{url}
\usepackage{indentfirst}
% Package used for \author{} command. Feel free to remove
% if it doesn't suits your needs.
\usepackage[affil-it]{authblk}
%\usepackage{hyperref}
\usepackage[numbers, square, sort]{natbib}
\usepackage{subfig}
\usepackage{fixltx2e}
\usepackage{textcomp}
\usepackage{float}
\usepackage{floatflt}

\pagestyle{empty}

\newcommand{\engl}[1]{(engl.~\emph{#1})}

\setlength{\topskip}{0cm}

% List spaceing.
\renewcommand{\labelitemi}{\textendash}
\renewcommand{\labelitemii}{\textbullet}
\renewenvironment{itemize}{%
\begin{list}{\labelitemi}{%
\setlength{\topsep}{0mm}
\setlength{\itemsep}{-1mm}
\setlength{\labelindent}{\parindent}
\setlength{\leftmargin}{6mm}}}
{\end{list}}

% Setting right title format.
\let\LaTeXtitle\title
\renewcommand{\title}[1]{\LaTeXtitle{\Large \textbf{#1}}}
\renewenvironment{abstract}
{\noindent \large \bf Abstract. \normalsize \begin{it}}
{\end{it}\\}

% Adding dots after {sub}secions.
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\large\bfseries}{\thesubsubsection.}{1em}{}

\newenvironment{keywords}
{\noindent {\large {\bf Keywords}}.~}{}

% Space between authors-(authblk package)
\renewcommand\Authsep{ \quad }
\renewcommand\Authand{ \quad }
\renewcommand\Authands{ \quad }

% Space between affils.
\setlength{\affilsep}{0em}

% Bibliography
\renewcommand{\bibsection}{\section{References}}
\setlength{\bibsep}{2mm}

% Environment for tables inside of multicol
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}
\makeatother


\begin{document}

\title{Automatic authorship attribution for Croatian texts}
\author{Tomislav Reicher}
\author{Ivan Krišto}
\author{Igor Belša}
\author{Artur Šilić}
\affil{Faculty of Electrical and Computing Engineering, University of
Zagreb\\Unska 3, 10000 Zagreb, Croatia}
\affil{\{tomislav.reicher, ivan.kristo, igor.belsa, artur.silic\}@fer.hr}

% Removal of date. Don't change.
\date{}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}
\begin{multicols}{2}

% TODO:
% - Još malo popraviti predložak

\begin{abstract}
Automatic authorship attribution is a useful task in automatic text
classification which becomes quite complex if it is applied in large data sets
(such as application at web search engines). Most reported methods are language
specific, therefore we have developed methods specific for Croatian language
which range from simple, based on stylistic measures and functional words, to
complex which require syntactic analysis of the Croatian language. We have found
that various combinations of methods are very successful in solving the
authorship attribution problem.
\end{abstract}

% Please, insert keywords:
\begin{keywords}
text classification, SVM, functional words, MSD tagging.
\end{keywords}

\section{Introduction}
Automatic authorship attribution can be interpreted as a problem of text
classification based on linguistic features specific to certain authors. Problems similar to authorship
attribution are detection of author's age, region or gender
\citep{luyckx2005shallow}. The main concern in computer--assisted authorship
attribution is defining an appropriate characterization of the text. Such characterization should
capture the writing style of the authors \citep{coyotl2006authorship}.

% \citet{kukushkina2001using} notes that first works on authorship attribution
% problem are ``\emph{Izv.\ otd.\ russkogo jazyka i slovesnosti, Imp.\ akad.\ nauk.}'',
% Morozov, N., A.\ from 1915.\  and ``\emph{On some application of statistical
% method}'' from 1916.\ by Markov, A., A., founder of Theory of Markov chains.

Authorship attribution can help in document indexing, document filtering and
hierarchical categorization of web pages \citep{luyckx2005shallow}. These
applications are important tasks in the field of information retrieval. 
Authorship attribution differs from plagiarism detection. Plagiarism detection
attempts to detect similarities between two substantially different pieces of
work. However, it is unable to determine if they were produced by the same author or not
\citep{de2001mining}.

The problem of authorship attribution can be divided into three categories \citep{zhao2005effective}: binary,
multi--class and single--class (or one--class) classification. Binary classification 
solves the problem when the data set contains the texts written by one of two authors. Multi--class 
classification is a generalization of the binary classification when there are 
more than two authors in the data set. One--class classification
is applied when only some of the texts from the data set are written by a particular
author while the authorship of all the other texts is unspecified. This
classification ascertains whether a given text belongs to a single known author or not.

This paper presents a study of multi--class classification for texts in Croatian
language, oriented on the evaluation of different text representations. The rest of
the paper is organized as follows. Section 2 discusses related work on authorship
attribution and similar problems. Section 3 describes the used data set. Section
4 introduces methods of text representation we have used. Section 5 describes
classification and Section 6 presents evaluation methods and experiment results.
Conclusion and future work are given in Section 7.

\section{Related work}
There are several approaches to text representation and classification in the field 
of authorship attribution. This approaches include: stylistic measures \citep{coyotl2006authorship}, syntactic clues
\citep{stamatatos2001computer,uzuner2005comparative}, word-based features
\citep{argamon2005measuring,uzuner2005comparative}, compression algorithms
\citep{kukushkina2001using,zhao2005effective}, grammatical error lookup
\citep{koppel2003exploiting}, language modeling
\citep{peng2003language,coyotl2006authorship}, and vocabulary diversity
\citep{stamatatos2001computer}. Following paragraphs describe these approaches in
more depth and relate our work with the existing research.

\citet{coyotl2006authorship} group the methods of authorship attribution into three
main approaches: \emph{stylistic measures}, \emph{syntactic cues} and
\emph{word--based} text features.

Text features based on \emph{stylistic measures} are characterized by features that
take into account the length of words, the length of sentences and the richness of vocabulary.
Various types of style markers can be found in
\citep{luyckx2005shallow}. In \citep{coyotl2006authorship} it is noted that such
features are not sufficient to resolve problems dependent on the genre of the
text. Moreover, they lose their meaning when applied over short texts.

When using the \emph{syntactic cues as text features} --- information
related to structure of the language that is obtained by an in--depth syntactic
analysis of the text --- a single text is characterized by the presence and
frequency of certain syntactic structures. This characterization is detailed and relevant.
Unfortunately, it is computationally expensive or even impossible to build for
certain languages lacking the text--processing resources (e.g., POS tagger, syntactic
parser, etc.). This method is clearly influenced by the length of the text. More detailed 
description of the authorship attribution process based on syntactic elements can be found 
in a very influential work by Stamatos et al. \citep{stamatatos2001computer}.

The approach with \emph{Word--based} text features can be based on three
different types of methods. The first type of methods characterize a text
using a set of functional words (their presence and frequency). Content words, which 
tend to be highly correlated with the text topics, are ignored. The second type
of methods observe a single text as a bag--of--words and use single content words as
features. This type of methods can be applied only when there is a noticeable
correlation between the authors and the topics. The third type considers
word \emph{n}-grams as features, i.e., features consisting of sequences of
consecutive words.
% Nevertheless, due to the feature explosion, it tends to use
% only \emph{n}-grams up to three words.

The success of functional words over collocations (certain pairs of words
occurring within a given threshold distance of each other), is shown in
\citep{argamon2005measuring}. The opposite might mistakenly be concluded considering
the information reduction when using functional words as features.
\citet{argamon2005measuring} believe that the most of the discriminating power
of collocations is due to the frequent words they contain and not due to the
collocations themselves.
% They also note that using more training texts than
% features seriously reduces the likelihood of overfitting the model to the
% training data, improving the reliability of results. Similar claim can be found
% in \citep{banko2001scaling} which deals with influence the of corpus size in
% general natural language processing.

A similar comparison of feature types is shown in
\citep{uzuner2005comparative}. The functional words and syntactic
elements are compared in order to identify the author of a text. It is
concluded that the syntactic elements of expressions are as useful as functional 
words in solving the problem.

\citet{kukushkina2001using} explain a method that uses certain algorithms for data
compression in order to identify the author of a text. The appendix of this paper shows evaluation results
for authorship attribution with different compression algorithms.
% The idea behind
% the method is to divide texts by authors, compress every set with selected
% algorithm and write the size of the archive. To classify text of an unknown author,
% text is added to each set and than the same data compression algorithm is applied. The author of
% the texts in archive which records the smallest increase in size is declared
% as the author of the new text.
A review of a similar method can be found in 
\citep{zhao2005effective}. It is noted that the method has obvious omissions, 
and a citation on another work that proves this is given.

\citet{koppel2003exploiting} show the use of grammatical errors and informal
styling (e.g., writing sentences in capital letters) as text features used for authorship attribution.
The method is only applicable for unedited texts (blogs, Internet forums,
newsgroups, e--mail messages, etc.).

\citet{peng2003language} suggest the use of \emph{n}-gram language model to identify
the author. A similar method is shown in \citep{coyotl2006authorship}.

\citet{stamatatos2001computer} suggest vocabulary diversity as
features for identifying the authors. The diversity is defined as the ratio of unique words 
and the total size of the text. An alternative strategy is counting the words which occur only once
(\emph{hapax legomena}), twice (\emph{dis legomena}), etc.
These measures are closely related to the length of the text. They also note that
the functions defined by Yule (1944) and Honore (1979) should solve this problem,
i.e., they should be constant regarding the length of the text. The analysis of
similar functions can be found in the \citep{tweedie1998variable}. It claims
that most of these functions are not independent regarding the length of the
text. It is concluded that the vocabulary diversity is an unstable feature for
the texts shorter than 1000 words.

%TODO: prosiriti
Our work is based on the composition of various methods --- syntactic clues, word--based features, 
and stylistic measures --- adapted for use with the Croatian language. 

\section{Data set}
\label{sec:podatci}
%TODO: columns, kao prijevod za kolumnu je OK, ali mene uvijek prvo asocira na stupce!!!
% mislim da oni to nikada tako ne zovu u stvarnosti, nego articles/commentarys/postings... ne znam... PROVJERITI!
We used an online archive of articles (columns) from a daily Croatian newspaper ``Jutarnji
list'', available at \url{http://www.jutarnji.hr/komentari/}. The data set
consists of 4571 texts written by 25 authors. The lowest average number of words
in the text per author is 315 words, and the highest is 1347 words. An average number
of words in text per author for this data set is 717 words. The number of texts per
author in the set is shown on Figure \ref{fig:articlesPerAuthor}.

Since the topics in these articles tend to be time--specific, and to avoid the learning of topic, 
%FIXME Kako prevesti ucenje teme? ovo mi se ne cini ok!? 
we split the set by dates -- 20\% of the newest articles of
each author are taken for testing (hold--out method). Therefore, training data
set contains 3425 texts and testing data set 1146 texts.

\begin{minipage}{0.8\linewidth}
\vspace{10pt}
\centerline{\resizebox{1.4\linewidth}{!}{\input{figures/articlesPerAuthor}}}%
\figcaption{\small \textbf{\textsf{Number of texts per author.}}}%
\label{fig:articlesPerAuthor}
\end{minipage}

\section{Text representation}
When constructing an authorship attribution system, the central issuse is the selection of 
sufficiently discriminative features. A feature is discriminative if it is common for one author 
and rare for all the others. Due to the large number of authors some complex features are very useful if their
distribution is specific to each author. % (e.g., the frequency distribution of some words).%

%Features can be combined. If complex features are combined, new features can represent more distributions and have greater discriminatory power.%

During the creation of the feature vector, it is neccessary to make the
features independent of the length or of the content of each text. That
dependency reduces generality of system's application and can lead to a decrease of accurancy
(e.g., relating author to concrete topic or terms).

Since features are expressed as real numbers, each text is expressed as a real
vector of an appropriate dimensions. We combine features simply by creating unions of basic feature sets.

% Depending on selected feature model, text analysis and creation of feature
% vectors can vary from computationally very trivial operations to very complex
% algorithms (such as obtaining syntactic features by analyzing natural language).


\subsection{Functional words frequency ($\mathcal{F}$)}
\label{sec:funkcijske-rijeci}
Functional words, such as adverbs, prepositions, conjunctions, or interjections,
are words that have little or no semantic content of their
own. They usually indicate a grammatical relationship or a generic property
\citep{zhao2005effective}.

% Usefulness of function words occurrence frequency for authorship attribution lies
% in the fact that they are the indicators of writing style.
Some of the less used functional words, such as prepositions \emph{onkraj},
\emph{namjesto} and \emph{zavrh} in the Croatian language, are rarely used and may very well suggest the
author. However, even the frequency of more common functional words can adequately distinguish the author. 
Due to the high frequency of the functional words and their significant roles in the grammar, the author 
usually has no conscious control over their usage in a particular text
\citep{argamon2005measuring}. They are also topic--independent.
Therefore, functional words are good indicators of the author's style.

It is difficult to predict whether these words will give equally good
results for different languages. Moreover, despite the abundance of research in this
field, due to various languages, types and sizes of the texts, it is
currently impossible to conclude if these methods are generally effective \citep{zhao2005effective}.

In addition to functional words, auxiliary verbs and pronouns are also considered.
Their frequencies might be representative for the styles of different authors.

The feature vector of a text is created by counting the appearances
of functional words, auxiliary verbs and pronouns in a text. The results
are written as vectors. Each component corresponds to the number of occurences of
related words divided by the total number of the words in the text. This is done
to remove the dependency on the length of the text. In the case of functional words that
have not appeared in a certain text, the resulting component always equals zero.

Label $\mathcal{F}$ denotes evaluation result of this feature in Table
\ref{tbl:eval}.

\subsection{Lexical categories frequency ($\mathcal{C}$)}
\label{sec:rijeci-grupe}
The feature vector is created by counting the appearances of
different lexical categories. The used categories are adverbs,
adpositions, conjunctions, particles, interjections, nouns, verbs, adjectives and
pronouns. They are obtained by syntactic analysis of the Croatian language. Result is
stored as a vector (nine--dimensional vector). Each component of the vector
is divided by the number of words in the text.

\subsection{Idf weighted functional words frequency ($\mathcal{I}$)}
\label{sec:funkcijske-rijeci-idf}
The problem of the text length dependency in the classification using
an SVM (Support Vector Machine) is explained in \citep{diederich2003authorship}.
%FIXME članovi skraćenica mogu biti kako se citaju ili za pune rijeci. Ovdje moze biti
% i a i an jer moze biti _a Support Vector_ ili _an EsViEm_.
To avoid the dependency, we used a combination of $L_p$ normalization of the
length and transformation of the terms occurrence frequency, such as \emph{idf}
(inverse document frequency) measure.

Idf measure is defined as \citep{diederich2003authorship}
\begin{equation}
F_{idf}(t_k) = \log \frac{n_d}{n_d(t_k)},
\label{equ:idf}
\end{equation}
where $n_d(t_k)$ is number of documents that contain term $t_k$ and
$n_d$ the total number of documents. The shown measure gives high values for terms
that appear in a small number of documents and is thus very discriminatory.
%FIXME Zasto term? originalno smo koristili rijec. Term jest _izraz_, ali nekako se rijede koristili
% u ovom smislu, a cesce u smislu _semestra_ ili _uvijeta_. Expression ili phrase za izraze u govornom jeziku...
%FIXME Ovdje namjerno koristimo documents umjesto texts?

The feature vector is built by multiplying components of the vector created by a method
defined in \ref{sec:funkcijske-rijeci} and its associated \emph{idf} weight.

The shown measure discriminates documents that contain functional words used in small 
number of other documents. The disadvantage of the \emph{idf} measure is
in the fact that it records only the presence of a certain term in a document. Term
counting is ignored. Therefore, a term that appears many times in
one single document and once in all the others gets the same value as the one which appears
once in all of the texts. Thus, the word which would very well separate one document from the
others is ignored.

\subsection{Punctation marks ($\mathcal{P}$), vowels ($\mathcal{V}$), words
length ($\mathcal{L}$) and sentence length ($\mathcal{S}$) frequency}
\label{sec:znacajke-manje}
A set of following punctuation marks is used: ``.'', ``,'', ``!'', ``?'',
``''', ``"'', ``-'', ``:'', ``;'', ``+'', ``*''. Their appearance in the text is counted and
the result is written as an 11--dimensional vector. Every
component is divided by the total number of characters in the text. 

A feature vector based on the frequency of vowel occurence (a, e, i, o, u) is
obtained in an equal manner.

The Frequency of words length is obtained by counting the words with equal
length. It is important to note that this procedure can lead to vectors with different dimensions
(e.g., a text has a word with length of 11, but some other text does not). 
The issue is solved by limiting the maximum word length to the length of 10. 
All the words longer than 10 characters are attributed to the group of length 10. 
It is necessary to divide the components of the resulting vector 
with the number of words in a text to diminish the dependency of features on the text 
length. 

The sentence length frequency is obtained in a similar procedure.
The different vector dimensions issue is solved by limiting the sentence length to 20.

Features suggested here all have weak discriminatory power on their own, However, they proved 
very useful in the combination with other features, as shown in Section \ref{sec:evaluacija}.

\subsection{Word part--of--speech \emph{n}-grams frequency ($\mathcal{N}_1$ \&
$\mathcal{N}_2$)}
\label{sec:ngrami-tipova}
The two proposed features are based on word part--of--speech \emph{n}-grams
frequency. Word parts--of--speech and their morphosyntactic descriptors are obtained by POS
(Part--Of--Speech) and MSD (morphosyntactic) tagging for Croatian language
\citep{snajder08automatic}. Having the corresponding part--of--speech, for
each word in the text, makes the idea of using \emph{n}-grams as features
possible. As \emph{n}-grams features can produce very large dimensionality 
only 3-grams are considered. In addition POS tagging used is not perfect, the
method does not use context information and therefore cannot distinguish between
different homographs---words with same spelling but with different meaning and probably
different POS too---so all possible POS tags for given word are considered.

First proposed feature uses the words parts--of-speech
(nouns, verbs, adjectives, pronouns, conjunctions, interjections and
prepositions) to form various \emph{n}-grams and count their frequencies. For
example, word 3-gram ``Adam i Eva'' (``Adam and Eve'') forms ``noun
conjunction noun'' trigram. Second proposed feature uses only words
parts--of-speech information for nouns, verbs and adjectives and for other word
parts--of-speech it uses words as they are, therefore ``Adam i Eva'' transforms
to ``noun i noun''. Due to many different pronouns, conjunctions, interjections
and prepositions that make many different \emph{n}-grams, frequency filtering
is applied---only frequency of 500 most frequent 3-grams in
training data set is considered. Used dimension reduction method is not optimal,
therefore in future work other methods should be evaluated, such as information
gain, $\chi^2$ test, mutual information, maximum relevance, minimum redundancy
or classification with sparse SVM, logistic regression or na\"ive Bayes.

\subsection{Word morphologic categories ($\mathcal{M}$)}
\label{sec:morphosyntactic}
Building of feature vector is done by counting appearances of morphologic
categories for every word in text and dividing them by number of words in text.
Counted morphologic categories are \emph{case}, \emph{degree}, \emph{form},
\emph{gender}, \emph{number} and \emph{person}.

\end{multicols}
\begin{table*}[htb]
\begin{center}%
\caption{\small \textbf{\textsf{Evaluation of different features}}}%
\begin{tabular}{r c c r@{.}l}%
\toprule%
Method & Accuracy [\%] & $C$ & \multicolumn{2}{c}{$\gamma$} \\
\midrule
$\mathcal{F}$ & 88.39 & 8192 & 0 & 125\\
$\mathcal{I}$ & 87.96 & 8192 & 0 & 125\\
$\mathcal{C}$ & 44.50 & 512 & 2 & 0\\
$\mathcal{P}$ & 57.50 & 8192 & 0 & 125\\
$\mathcal{V}$ & 30.54 & 128 & 0 & 125\\
$\mathcal{L}$ & 43.19 & 128 & 0 & 125\\
$\mathcal{S}$ & 42.32 & 128 & 0 & 125\\
$\mathcal{N}_1$ & 71.29 & 512 & 0 & 125\\
$\mathcal{N}_2$ & 76.09 & 512 & 0 & 125\\
$\mathcal{M}$ & 61.17 & 512 & 0 & 125\\
$\mathcal{C}$, $\mathcal{M}$ & 63.17 & 8192 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$ & 91.71 & 8 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{M}$ & 91.18 & 128 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{N}_1$ & 89.44 & 128 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{N}_2$ & 88.48 & 128 & 0 & 03125\\
$\mathcal{I}$, $\mathcal{M}$ & 90.84 & 128 & 0 & 03125\\
$\mathcal{N}_1$, $\mathcal{M}$ & 71.38 & 128 & 0 & 03125\\
$\mathcal{I}$, $\mathcal{M}$, $\mathcal{C}$ & 91.36 & 128 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$ & \textbf{93.11} & 128 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$, $\mathcal{M}$ & \textbf{93.46} & 32768 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{M}$, $\mathcal{C}$, $\mathcal{N}_1$ & 89.62 & 128 & 0 & 03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$ & \textbf{93.19} & 128 & 0 & 03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$, $\mathcal{N}_1$ & 92.41 & 128 & 0 & 03125\\
\bottomrule%
\end{tabular}%
\label{tbl:eval}%
\end{center}
\end{table*}
\begin{multicols}{2}

\section{Classification}
% Representation of documents by real number vectors  enables easy use of
% classifiers that search decision functions, i.e., boundaries in vector
% space.
The classifier used is SVM (Support Vector Machine) with radial basis
function as the kernel. It is shown that, with the use of parameter selection,
linear SVM is special case of SVM with RBF kernel \citep{keerthi2003asymptotic}
what removes the need to consider linear SVM as potential classifier.

Before the use of SVM, it is required to scale the data to ensure
equal contribution of every attribute to the classification. Components of
every feature vector are scaled to interval $[0, 1]$.
%  according to following expression:
% \begin{equation}
% x^{s}_{i,j} = \frac{x_{i,j} - \min_{i}\; x_{i,j}}{\max_{i}\; x_{i,j}
% - \min_{i}\; x_{i,j}}
% \end{equation}
% where $x^{s}_{i,j}$ is scaled component $j$ of vector $\mathbf{x_i}$,
% $\min_{i}\; x_{i,j}$ is minimum value of attribute $j$ among all vectors
% $\mathbf{x_i}$ and $\max_{i}\; x_{i,j}$ is maximum value of attribute $j$ among
% all vectors. If we denote the resulting minimum and maximum values as follows:
% \begin{eqnarray}
% M_i & = \max_{i}\; x_{i,j} \\
% m_i & = \min_{i}\; x_{i,j}
% \end{eqnarray}
% then, the unknown vector $\mathbf{x}$ before classification is scaled as:
% \begin{equation}
% x^{s}_{j} = \frac{x_j-m_i}{M_i-m_i}
% \end{equation}

% TODO: REPHRASE
Finding appropriate paramters $(C, \gamma)$ is done by the means of
cross-validation: using the 5-fold cross--validation on the
learning set parameters $(C, \gamma)$ which give the highest accuracy are
selected. Parameters that were considered are: $C \in \{2^{-5}, 2^{-4}, \cdots ,
2^{15}\}$, $\gamma \in \{2^{-15}, 2^{-14}, \cdots, 2^3\}$ \citep{CC01a}.
The accuracy of classification is measured by the expression:
\begin{equation}
acc = \frac{n_c}{N}, % ako bude problema, ovo staviti kao ``inline'' jednadžbu.
\end{equation}
where $n_c$ is number of correctly classified texts, $N$ is total number of
texts. After the best parameters  $(C, \gamma)$ are found for
which the system achives the highest accuracy, the classifier is learned using
those parameters.

\section{Evaluation}
\label{sec:evaluacija}
Classification success is measured by ratio of correctly classified texts and
total number of texts (accuracy). Results of evalutaion are shown in Table
\ref{tbl:eval}. Columns ``$C$'' and ``$\gamma$'' denote parameters of SVM
classifier used for selected model.

% Highest accuracy during cross--validation is achived by combination of
% methods marked as $\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$,
% $\mathcal{L}$ and $\mathcal{M}$.

%% TODO: Što s ovim?
% Total accuracy doesn't explains behaviour of classifier for every class by
% itself. For every class precision and recall are calculated and by them we
% calculate total weighted $F$ measure. For particular class $c$, precision is
% ratio of number of correctly classified documents in $c$ with number of all
% documents which are classified as $c$. Recall is ratio of number of correctly
% classified documents in $c$ with number of all documents in $c$. $F$ measure is
% calculated for every class $c_i$ according to following expression:
% \begin{equation}
% F_i = \frac{2 \cdot precision_i \cdot recall_i}{precision_i + recall_i}
% \end{equation}
% where $precision_i$ and $recall_i$ are measures of precision and recall for
% class $c_i$.
% 
% Total weighted measure is calculated by following expression:
% \begin{equation}
% F_u = \frac{\sum^{n}_i |c_i|\cdot F_i}{\sum^n_i|c_i|}
% \end{equation}
% where $n$ is total number of classes, $|c_i|$ number of documents in class
% $c_i$ and $F_i$ a $F$ measure for class $c_i$.
% 
% Weighted $F$ measure at our data set for testing gives value of 87\%.

\section{Conclusion}
It is shown that authorship attribution problem can be successfully solved by
relatively simple methods, although for state--of--the--art methods, we
believe that syntactic analysis of texts is required. Our results with
93\% accuracy fit very well in interval of reported results which range from
70\% to 97\% \citep{coyotl2006authorship,keselj2003n,luyckx2005shallow,stamatatos2001computer}.

Result comparision is difficult due to different types of data sets (e.g., poems,
newspaper articles, e--mails) and the problems (binary, multi--class or
single--class classifications). There are no relevant data sets for comparision
\citep{zhao2005effective}, but there are references on paper
\citep{stamatatos2001computer} and their ``Greek data set'' (e.g.,
\citep{keselj2003n}). There is an significant affected of data set type (number
and variety of text samples) on the complexity of the problem.

In future work, methods based on word and character \emph{n}-grams suggested
in \citep{keselj2003n,peng2003language,coyotl2006authorship} has to be
evaluated. Also, evaluation has to be performed on different types of data sets
such as poems, newspaper articles or books data sets.

\section*{Acknowledgement}
% TODO: Napisati

\bibliographystyle{plainnat}
\bibliography{literatura}

\end{multicols}

\end{document}
