\documentclass[11pt,english]{article}

\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm, columnsep=0.6cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{nonfloat}
\usepackage{url}
\usepackage{indentfirst}
% Package used for \author{} command. Feel free to remove
% if it doesn't suits your needs.
\usepackage[affil-it]{authblk}
%\usepackage{hyperref}
% Da bi se promjenio stil citiranja umjesto:
% [authoryear, round]
% staviti:
% [numbers, square]
\usepackage[numbers, square]{natbib}
\usepackage{subfig}
\usepackage{fixltx2e}
\usepackage{textcomp}
\usepackage{float}
\usepackage{floatflt}

\pagestyle{empty}

\newcommand{\engl}[1]{(engl.~\emph{#1})}

% Changeing list spaceing.
\renewcommand{\labelitemi}{\textendash}
\renewcommand{\labelitemii}{\textbullet}
\renewenvironment{itemize}{%
\begin{list}{\labelitemi}{%
\setlength{\topsep}{0mm}
\setlength{\itemsep}{-1mm}
\setlength{\labelindent}{\parindent}
\setlength{\leftmargin}{6mm}}}
{\end{list}}

% Setting right title format.
\let\LaTeXtitle\title
\renewcommand{\title}[1]{\LaTeXtitle{\Large \textbf{#1}}}
\renewenvironment{abstract}
{\noindent \large \bf Abstract. \normalsize \begin{it}}
{\end{it}\\}

% Adding dots after {sub}secions.
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\large\bfseries}{\thesubsubsection.}{1em}{}

\newenvironment{keywords}
{\noindent {\large {\bf Keywords}}.~}{}

% Putting space between authors-(authblk package)
\renewcommand\Authsep{ \quad }
\renewcommand\Authand{ \quad }
\renewcommand\Authands{ \quad }

% Removal of space between affils.
\setlength{\affilsep}{0em}

\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}
\makeatother


\begin{document}

\title{Automatic authorship attribution for Croatian texts}
\author{Igor Belša}
\author{Tomislav Reicher}
\author{Ivan Krišto}
\author{Artur Šilić}
\affil{Faculty of Electrical and Computing Engineering, University of
Zagreb\\Unska 3, 10000 Zagreb, Croatia}
\affil{\{igor.belsa, tomislav.reicher, ivan.kristo, artur.silic\}@fer.hr}

% Removal of date. Don't change.
\date{}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}
\begin{multicols}{2}

% TODO:
% - Napisati abstract
% - Srediti literaturu (prilagoditi je ITI pravilima)
% - Još malo popraviti predložak
% - Izbaciti kolumnista s malo članaka.

% Jedna _vrlo kratka_ lista glagola koje je poželjno koristiti pri pisanju članka:
% {notes, explains, references, shows, proves, defines, believe, convince,
% suggest}.

\begin{abstract}
The abstract is to be in fully-justified italicized text, at the top of the
left-hand column as it is here, below the author information. Use the word
``Abstract.'' in 12--point Times, boldface type, left positioned, initially
capitalized, followed by the abstract in 11--point, single--spaced type, up to
100 words long.

Leave one blank line after the abstract, then begin the keywords. Use
the word ``Keywords.'' in 12--point Times, boldface type, left positioned,
initially capitalized, followed by up to ten keywords in 11--point, separated
by comma, as below.

Leave one blank line after the keywords, then begin the main text.
\end{abstract}

% Please, insert keywords:
\begin{keywords}
Times New Roman, 11 pt.
\end{keywords}

\section{Introduction}
Automatic authorship attribution can be interpreted as problem of text
classification based on its lingvistic features. Problems similar to authorship
attribution are detection of age, region and gender of the author
\citep{luyckx2005shallow}. Main concern of computer--assisted authorship
attribution is to define an appropriate characterization of document that
captures the writing style of authors \citep{coyotl2006authorship}.

\citet{kukushkina2001using} notes that first works on authorship attribution
problem are ``\emph{Izv.\ otd.\ russkogo jazyka i slovesnosti, Imp.\ akad.\ nauk.}'',
Morozov, N., A.\ from 1915.\  and ``\emph{On some application of statistical
method}'' from 1916.\ by Markov, A., A., founder of Theory of Markov chains.

Authorship attribution can help in document indexing, document filtering and the
hierarchical categorization of web pages and web search engines
\citep{luyckx2005shallow}. It is important to note that authorship attribution is
different from plagiarism detection. Plagiarism detection attempts to detect the
similarity between two substantially different pieces of work but is unable to
determine if they were produced by the same author \citep{de2001mining}.

The problem can be divided in three categories \citep{zhao2005effective}: binary,
multi--class and single--class (or one--class) classification. Binary
classification solves the problem when each document from data set is written by
one of two authors. Multi--class classification is generalization of binary
classification in which we have more than two authors. Single--class
classification is applied when some of the documents from data set are written by
a particular author while the authorship of the other documents is unspecified.
This classification answers the question about a given document belonging to
single known author or not.

This paper presents a study of the problem of multi--class classification for
documents in Croatian language, oriented on evaluation of different sets of
documents features.

The rest of the paper is organized as follows. Section 2 discusses some previous
works on authorship attribution and similar problems. Section 3 describes used
data set. Section 5 is the most important, and it introduces used methods.
Section 6 describes classification and 7th section discusses evaluation and
gives evaluation results. Conclusion and future work is given in 8th section.

\section{Related work}
\citet{coyotl2006authorship} cluster methods of authorship attribution in three
main approaches:
\begin{description}
\item[Stylistic measures as document features:] Characteristic features for this
approach take into account length of words and sentences and the richness of the
vocabulary. \citet{coyotl2006authorship} notes that such features are not
sufficient to resolve problems that depend on the genre of the text and that they
lose their meaning when applied over short texts.

\item[Syntactic cues as document features:] Using the author style markers as
features -- informations related to structure of the language which are obtained
by an in--depth syntactic analysis of documents. Text is characterized by the
presence and frequency of certain syntactic structures. This characterization is
detailed and relevant. Unfortunately, it is computationally expensive and even
impossible to build for languages lacking of text--processing resources (e.g.\
POS tagger, syntactic parser, etc.). It is clearly influenced by the length of
documents.

\item[Word--based document features:] This approach branches in three different
methods. \emph{The first one} characterizes documents using a set of functional
words (their presence and frequency), ignoring the content words since they tend
to be highly correlated with the document topics. The method shows good results,
but it is affected by the size of the document. \emph{Second branch} of methods
observes document as a bag--of--words and uses single content words as document
features. Method is very robust and produces excellent results when there is a
noticeable correlation between the authors and the topics. The \emph{third
branch} considers word $n$--grams as features, i.e.\ features consisting of
sequences of consecutive words. Nevertheless, due to the feature explosion, it
tends to use only $n$--grams up to three words.
\end{description}

\citet{kukushkina2001using} explain method which uses algorithms for data
compression to identify the author. Appendix of paper shows evaluation results
for authorship attribution with different compression algorithms. The idea behind
the method is to divide documents by authors and compress every set with selected
algorithm and write size of archive. To classify text of unknown author, text is
added to each set and the same data compression algorithm is applied. Author of
documents in archive which records the smallest increase of the size is declared
as author of the new document. Review of a similar method can be found in the
paper \citep{zhao2005effective}. Paper notes that the method has obvious
omissions and cites paper which proves it.

\citet{koppel2003exploiting} show the use of gramatical errors and informal
styling (e.g.\ writing sentences in capital letters) as document features in
order to identify the author. Method is applicable only to unedited texts (blogs,
Internet forums, newsgroups, e--mail messages, etc.).

\citet{peng2003language} suggest the use of $n$--gram language model to identify
the author. A similar method is shown in \citep{coyotl2006authorship}.

The success of functional words over collocations (certain pair of words
occurring within a given threshold distance of each other) is shown in
\citep{argamon2005measuring} -- although, because of information reduction at
using function words as features, one might conclude the opposite -- which is
wrong. \citet{argamon2005measuring} believe that most of the discriminating power
of collocations is due to the frequent words they contain (and not the
collocations themselves). They also note that using more training texts than
features seriously reduces the likelihood of overfitting the model to the
training data, improving the reliability of results. Similar claim can be found
in \citep{banko2001scaling} which deals with influence of the corpus size in
general natural language processing.

A similar comparison of the feature types is shown in
\citep{uzuner2005comparative} where the function words and syntactic
elements are compared in order to identify the author of document. It is
concluded that the syntactic elements of expression are useful as functional words in solving the
problem.

Description of the authorship attribution process by using syntactic elements
of language can be found in the extremely influential work
\citep{stamatatos2001computer}. Various types of style markers can be found in
\citep{diri2003automatic,luyckx2005shallow}.

\citet{stamatatos2001computer} suggest vocabulary diversity of the authors as
feature for identifying the author. It is measured by the ratio of unique words
and the total size of the text or counting words which occur only once
(\emph{Hapax legomena}), words which occur only twice (\emph{dis legomena}), etc.
These measures are closely related to length of the text. They also note that
functions defined in \citep{yule1944statistical,honore1979some} should solve this
problem, i.e.\ they should be constant regarding the length of the text. Analysis
of similar function can be found in the \citep{tweedie1998variable} which claims
that most of these functions are not independent regarding the length of the
text. It is concluded that the diversity of vocabulary is unstable feature for
texts shorter than 1000 words.

The problem of dependency on the length of the text for the classification using
a SVM (Support Vector Machines) is explained in \citep{diederich2003authorship}.
To avoid the dependency, a combination of $L_p$ normalization of the
length and transformation of terms occurrence frequency such as \emph{idf}
(inverse document frequency) measure was used.

Idf measure is defined as
\begin{equation}
F_{idf}(t_k) = \log \frac{n_d}{n_d(t_k)},
\label{equ:idf}
\end{equation}
where $n_d(t_k)$ is number of documents which contain term $t_k$ and $n_d$
total number of documents. Shown measure gives high values for terms which
appear in a small number of documents and thus it is very discriminatory.

\section{Data set}
\label{sec:podatci}
The online archive of ``Jutarnji list'' newspaper columns available at
\url{http://www.jutarnji.hr/komentari/} was used as data set. The data set
consists of 4571 texts written by 25 authors. The lowest average number of words
in document per author is 315 words, and the highest is 1347 words. Standard
average number of words in document per author for data set is 717 words. Number
of documents per author in data set is shown on figure
\ref{fig:articlesPerAuthor}.

To make sure that learning of topic is avoided, data set is splitted by dates
-- 20\% of newest texts of every author is taken for testing (hold--out method).

\begin{minipage}{0.8\linewidth}
\vspace{10pt}
\centerline{\resizebox{1.4\linewidth}{!}{\input{figures/articlesPerAuthor}}}%
\figcaption{\small \textbf{\textsf{Number of documents per author.}}}%
\label{fig:articlesPerAuthor}
\end{minipage}

\section{Document features}
The main problem of the construction of authorship attribution systems is the
selection of the sufficiently discriminatory features of the text. Feature is
discriminatory if it is common for one author, and rare for others. Due to the
large number of authors, complex features are extremely useful if their
distribution is characteristic for each author (e.g.\ the frequency distribution
of some words).

Features can be combined. If complex features are combined, new feature can
represent more distributions and have greater discriminatory power.

During the creation of the feature vector, it is neccessary to take care that the
feature does not depend on length or content of document. That dependency reduces
generality of system's application and can lead to decreasment of accurancy
(e.g.\ relating author to concrete topic or terms).

Features can be expressed as vector of real numbers which makes classification
very easy. Set of documents is expressed as set of vectors with equal dimensions.
Combining the features is expressed as connecting (appending) of their feature
vectors.

Depending on selected feature model, text analysis and creation of feature
vectors can vary from computationally very trivial operations to very complex
algorithms (such as obtaining syntactic features by analyzing natural language).


\subsection{Function words frequency}
\label{sec:funkcijske-rijeci}
% značenje, poput priloga, prijedloga, veznika, čestica, uzvika ili riječi koje
% TODO: Mislim da sam krivo preveo vrste riječi!
Function words are words that have little (or none) semantic content of their
own, like adverbs, prepositions, conjunctions, or articles. They usually indicate
a grammatical relationship or generic property \citep{zhao2005effective}.

Usefulness of function words occurrence frequency for authorship attribution lies
in the fact that they are the indicators of writing style. Some of the less known
function words, such as prepositions \emph{onkraj}, \emph{namjesto} and
\emph{zavrh} are rarely used, and may very well suggest who are the authors.
However, even the frequency of use of frequent functional words can be very well
used to distinguish the author. It is shown that because of the high frequency of
use of function words and their roles in the grammar, the author usually has no
conscious control over their use in a particular text
\citep{argamon2005measuring}.

Functional words are topic--independent, authors automatically (without conscious
control) use functional words which indicate their own style. During the changing
of the language of documents, it is also neccessary to update the set of
functional words with which the system works. It is difficult to predict how will
the changed system behave, and whether the function words may give equally good
results for different languages. Although there are more researches on this
topic, due to various languages, type and size of documents, it is difficult to
conclude are these methods generally effective \citep{zhao2005effective}.

Building the document feature vector is done by counting the number of appearance
of every functional word in the document. The results are written as vector (for
the function words that have not appeared in a document, the result is 0) . After
counting, each frequency is divided with the total number of words in the
document (to remove dependency on the length of the document).

\subsection{Function words types frequency}
\label{sec:funkcijske-rijeci-grupe}
% Ulazni skup funkcijskih riječi podijeljen je na: priloge, prijedloge, veznike,
% čestice i uzvike.
% TODO: U zagradama navesti tipove riječi!
Building of the feature vector is done by counting the number of appearance of
types of functional words. Result is written as a vector (five-- dimensional
vector) and each component of the vector is divided by the number of words in the
document.

\subsection{Idf weighted function words frequency}
\label{sec:funkcijske-rijeci-idf}
The feature vector is built by multiplying components of vector created by method
defined in \ref{sec:funkcijske-rijeci} and its associated \emph{idf}
\engl{inverse document frequency} weight. \emph{Idf} weight is defined by
expression (\ref{equ:idf}) \citep{diederich2003authorship}.

Shown measure discriminate documents that contain functional words that are used
in small number of other documents. The disadvantage of \emph{idf} measure is
that it makes record only of presence of certain term in the document, term
counting in the document is ignored. Therefore, word which appears many times in
one text and one time in the others gets the same value as the one which appears
once in all text---the word which would very well separate one document from the
others is ignored.

\subsection{Punctation marks, vowels and words length frequency}
\label{sec:znacajke-manje}
A set of following punctuation marks is used: ``.'', ``,'', ``!'', ``?'', ``''',
``"'', ``-'', ``:``, ``;'', ``+'', ``*'' and their number of appearance in
document is counted. Result is written as 11-dimensional vector and every
component is divided by total sum of characters in the document.

Feature vector based on the frequency of vowels occurence (a, e, i, o, u) is
obtained in equal procedure as for the punctuation marks

Frequency of word lengths are obtained by counting the words that have equal
length. It is important to note that this procedure can lead to vectors of
different features dimensions (e.g.\ a document has a word with length of 11,
but some other document doesn't). The issue is solved by limiting the maximum
length of words at length of 10. All words longer than 10 characters are
counted to the 10th group. It is necessary to divide components of resulting feature
vector with number of words in document (to remove dependency on text length).

Suggested features have weak discriminatory power on their own, but they are
very useful in combination with other features (see \ref{sec:evaluacija}).

\section{Classification}
Representation of documents by real number vectors  enables easy use of
classifiers that search decision functions, i.e.\ boundaries in vector
space.

For classification we have used SVM (Support Vector Machine) with radial basis
function as kernel. It is shown that, with selection of right parameters, linear
SVM is special case of SVM with RBF kernel \citep{keerthi2003asymptotic} which
removes need to use linear SVM as potential classifier.

It is required to scale data for use with SVM to ensure equal contribution of
every attribute to classification. We scale components of every feature vector
to interval of $[0, 1]$ according to following expression:
\begin{equation}
x^{s}_{i,j} = \frac{x_{i,j} - \min_{i}\; x_{i,j}}{\max_{i}\; x_{i,j}
- \min_{i}\; x_{i,j}}
\end{equation}
where $x^{s}_{i,j}$ is scaled component $j$ of vector $\mathbf{x_i}$,
$\min_{i}\; x_{i,j}$ is minimum value of attribute $j$ among all vectors
$\mathbf{x_i}$ and $\max_{i}\; x_{i,j}$ is maximum value of attribute $j$ among
all vectors. If we denote the resulting minimum and maximum values as follows:
\begin{eqnarray}
M_i & = \max_{i}\; x_{i,j} \\
m_i & = \min_{i}\; x_{i,j}
\end{eqnarray}
then, the unknown vector $\mathbf{x}$ before classification is scaled as:
\begin{equation}
x^{s}_{j} = \frac{x_j-m_i}{M_i-m_i}
\end{equation}

Parameter search is done by model selection with parameters $(C, \gamma)$ from
set $\left (C = {2^{-5}, 2^{-4}, \ldots , 2^{15}},  \gamma = {2^{-15}, 2^{-14},
\ldots, 2^3} \right )$ \citep{CC01a} which give the highest accuracy of
classification in process of cross--validation on learning data set. Accuracy
of classification is measured by expression:
\begin{equation}
acc = \frac{n_c}{N},
\end{equation}
where $n_c$ is number of correctly classified documents, $N$ is total number of
documents. After we find parameters $(C, \gamma)$ for which system has highest
accuracy, we learn classifier with given parameters. Parameters used in this
paper are $C = 16$ and $\gamma = 0.25$.


\section{Evaluation}
\label{sec:evaluacija}
Data set is splitted on set for learning and set for testing by taking 20\%
documents of each of 25 authors for testing. Data set for testing consists of
1146 documents. Classification success is measured by ratio of correctly
classified documents and total number of documents (accuracy).

Results of evalutaion are shown in \ref{tbl:eval}. For shorter record, method
defined in \ref{sec:funkcijske-rijeci} is marked as ``$\mathcal{F}$'', method
defined in \ref{sec:funkcijske-rijeci-grupe} is marked as ``$\mathcal{G}$'',
method from \ref{sec:funkcijske-rijeci-idf} ``\emph{idf}'' and methods from
\ref{sec:znacajke-manje} respectivly ``$\mathcal{P}$'', ``$\mathcal{V}$'' and
``$\mathcal{L}$''. Column ``Correct'' denotes number of correctly
recognized authors, ``Incorrect'' -- incorrectly recognized.

Highest accuracy (88\%) is achived by combination of every methods defined in
\ref{sec:znacajke-manje} and method from \ref{sec:funkcijske-rijeci}.

Total accuracy doesn't explains behaviour of classifier for every class by
itself. For every class precision and recall are calculated and by them we
calculate total weighted $F$ measure. For particular class $c$, precision is
ratio of number of correctly classified documents in $c$ with number of all
documents which are classified as $c$. Recall is ratio of number of correctly
classified documents in $c$ with number of all documents in $c$. $F$ measure is
calculated for every class $c_i$ according to following expression:
\begin{equation}
F_i = \frac{2 \cdot precision_i \cdot recall_i}{precision_i + recall_i}
\end{equation}
where $precision_i$ and $recall_i$ are measures of precision and recall for
class $c_i$.

Total weighted measure is calculated by following expression:
\begin{equation}
F_u = \frac{\sum^{n}_i |c_i|\cdot F_i}{\sum^n_i|c_i|}
\end{equation}
where $n$ is total number of classes, $|c_i|$ number of documents in class
$c_i$ and $F_i$ a $F$ measure for class $c_i$.

Weighted $F$ measure at our data set for testing gives value of 87\%.

% TODO: Smisliti kako srediti da je naslov tablice uvijek iznad tablice..
% \begin{minipage}{\linewidth}
\begin{tablehere}
\centering%
\caption{\small \textbf{\textsf{Evaluation of different features}}}%
\begin{tabular}{l c c c}
\hline\hline
Method & Correct & Incorrect & Accuracy \\
[0.5ex]
\hline
$\mathcal{P}$ & 603 & 489 & 55.2\% \\
$\mathcal{F}$ & 870 & 222 & 79.6\% \\
\emph{idf} & 872 & 220 & 79.8\% \\
$\mathcal{G}$ & 379 & 713 & 34.7\% \\
$\mathcal{V}$ & 326 & 766 & 29.8\% \\
$\mathcal{L}$ & 463 & 629 & 42.3\% \\
$\mathcal{V}$, \emph{idf} & 887 & 205 & 81.2\% \\
$\mathcal{F}$, \emph{idf} & 848 & 244 & 77.6\% \\
$\mathcal{P}$, $\mathcal{V}$, \emph{idf} & 948 & 144 & 86.8\% \\
$\mathcal{P}$, $\mathcal{V}$, $\mathcal{L}$, \emph{idf} & 955 & 137 & 87.4\% \\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, \emph{idf} & 905 & 187 & 82.8\% \\
$\mathcal{P}$, $\mathcal{F}$, \emph{idf} & 900 & 192 & 82.4\% \\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$ & 956 & 136 & 87.5\% \\
\textbf{$\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$} & \textbf{961} & \textbf{131} & \textbf{88.0\%} \\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, \emph{idf}, $\mathcal{L}$ & 913 & 179 & 83.6\% \\
%funkcijske, grupe & 872 & 220 & 79.8\% \\
%grupe, idf & 874 & 218 & 80.0\% \\
$\mathcal{P}$, $\mathcal{G}$, $\mathcal{V}$, \emph{idf} & 943 & 149 & 86.3\% \\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, \emph{idf}, $\mathcal{L}$, $\mathcal{G}$ & 912 & 180 & 83.5\% \\ [1ex]
\hline
\end{tabular}
\label{tbl:eval}%
\end{tablehere}%
% \end{minipage}

\section{Conclusion}
% TODO: Ovo će se ionako morati ponovo pisati (ili dosta mijenjati, pa
% ostavljam za kasnije).
Pokazano je da se problem automatskog prepoznavanja autora teksta može izuzetno
uspješno riješiti relativno jednostavnim metodama. U usporedbi s prijavljenim
rezultatima (od 70\% do 97\%
\citep{coyotl2006authorship,keselj2003n,luyckx2005shallow,stamatatos2001computer,stamatatos1999automatic}),
dobiveni rezultat (88\%) je izuzetno dobar. U obzir se mora uzeti variranje
rezultata zbog razlika u načinu evaluacije, podatcima nad kojima se evaluacija
vršila te samom problemu (binarna, višerazredna ili jednorazredna
klasifikacija).

Nažalost, ispravna usporedba s drugim radovima zasad nije moguća jer ne postoji
relevantan skup za usporedbu kao što navode \citep{zhao2005effective}, no u
literaturi se može naći pozivanje na rad
\citep{stamatatos2001computer,stamatatos1999automatic} i ``Grčki skup''
(npr.\ \citep{keselj2003n}). Na složenost problema bitno utječe broj autora i
raznolikost skupa uzoraka.

U daljnjem radu potrebno je evaluirati metode temeljene na $n$-gramima riječi i
slova poput onih opisanih u
\citep{keselj2003n,peng2003language,coyotl2006authorship} te korištenje oznaka
stila autora \engl{style markers} kao značajki – informacije vezane uz strukturu
jezika koje su skupljene dubinskom analizom sintaksne analize dokumenta
\citep{stamatatos2001computer,diri2003automatic,luyckx2005shallow}.

Uz navedeno, potrebno je provesti evaluaciju nad ispitnim korpusima različitih
prosječnih duljina tekstova (npr.\ pjesme, novinski članci, knjige).

% TODO: Prilagoditi ITI stilu
\bibliography{literatura}
\bibliographystyle{plainnat}

% \section{Detalji evaluacije najbolje značajke}
% \label{sec:detalji-evaluacije}
% \begin{itemize}
%   \item Kombinacija svih metoda iz \ref{sec:znacajke-manje} i metode iz
%      \ref{sec:funkcijske-rijeci},
%   \item broj autora	u skupu: 25,
%   \item ukupno uzoraka za učenje: 3267,
%   \item ukupno uzoraka za testiranje: 1092,
%   \item točnost: 0.88003665 (961/131),
%   \item F mjera: 0.8712441.
% \end{itemize}

\end{multicols}

\end{document}
