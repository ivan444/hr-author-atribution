\documentclass[11pt,english]{article}

\usepackage[a4paper, hmargin=2.5cm, vmargin=2.5cm, columnsep=0.6cm, textheight=24.7cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{babel}
\usepackage[T1]{fontenc}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{longtable}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{times}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{nonfloat}
\usepackage{url}
\usepackage{indentfirst}
% Package used for \author{} command. Feel free to remove
% if it doesn't suits your needs.
\usepackage[affil-it]{authblk}
%\usepackage{hyperref}
\usepackage[numbers, square, sort]{natbib}
\usepackage{subfig}
\usepackage{fixltx2e}
\usepackage{textcomp}
\usepackage{float}
\usepackage{floatflt}

\pagestyle{empty}

\newcommand{\engl}[1]{(engl.~\emph{#1})}

\setlength{\topskip}{0cm}

% List spaceing.
\renewcommand{\labelitemi}{\textendash}
\renewcommand{\labelitemii}{\textbullet}
\renewenvironment{itemize}{%
\begin{list}{\labelitemi}{%
\setlength{\topsep}{0mm}
\setlength{\itemsep}{-1mm}
\setlength{\labelindent}{\parindent}
\setlength{\leftmargin}{6mm}}}
{\end{list}}

% Setting right title format.
\let\LaTeXtitle\title
\renewcommand{\title}[1]{\LaTeXtitle{\Large \textbf{#1}}}
\renewenvironment{abstract}
{\noindent \large \bf Abstract. \normalsize \begin{it}}
{\end{it}\\}

% Adding dots after {sub}secions.
\titleformat{\section}{\large\bfseries}{\thesection.}{1em}{}
\titleformat{\subsection}{\large\bfseries}{\thesubsection.}{1em}{}
\titleformat{\subsubsection}{\large\bfseries}{\thesubsubsection.}{1em}{}

\newenvironment{keywords}
{\noindent {\large {\bf Keywords}}.~}{}

% Space between authors-(authblk package)
\renewcommand\Authsep{ \quad }
\renewcommand\Authand{ \quad }
\renewcommand\Authands{ \quad }

% Space between affils.
\setlength{\affilsep}{0em}

% Bibliography
\renewcommand{\bibsection}{\section{References}}
\setlength{\bibsep}{2mm}

% Environment for tables inside of multicol
\makeatletter
\newenvironment{tablehere}
  {\def\@captype{table}}
  {}
\makeatother


\begin{document}

\title{Automatic authorship attribution for Croatian texts}
\author{Tomislav Reicher}
\author{Ivan Krišto}
\author{Igor Belša}
\author{Artur Šilić}
\affil{Faculty of Electrical and Computing Engineering, University of
Zagreb\\Unska 3, 10000 Zagreb, Croatia}
\affil{\{tomislav.reicher, ivan.kristo, igor.belsa, artur.silic\}@fer.hr}

% Removal of date. Don't change.
\date{}

\maketitle

\thispagestyle{empty}
\pagestyle{empty}
\begin{multicols}{2}

% TODO:
% - Još malo popraviti predložak

\begin{abstract}
Automatic authorship attribution is a useful task in automatic text
classification which becomes quite complex if it is applied in large data sets
(such as application at web search engines). Most reported methods are language
specific, therefore we have developed methods specific for Croatian language
which range from simple, based on stylistic measures and functional words, to
complex which require syntactic analysis of the Croatian language. We have found
that various combinations of methods are very successful in solving the
authorship attribution problem.
\end{abstract}

% Please, insert keywords:
\begin{keywords}
text classification, SVM, functional words, MSD tagging.
\end{keywords}

\section{Introduction}
Automatic authorship attribution can be interpreted as a problem of text
classification based on linguistic features that are specific to certain authors. Problems similar to authorship
attribution are detection of age, region and gender of the author
\citep{luyckx2005shallow}. Main concern of computer--assisted authorship
attribution is to define an appropriate characterization of text that
captures the writing style of authors \citep{coyotl2006authorship}.

% \citet{kukushkina2001using} notes that first works on authorship attribution
% problem are ``\emph{Izv.\ otd.\ russkogo jazyka i slovesnosti, Imp.\ akad.\ nauk.}'',
% Morozov, N., A.\ from 1915.\  and ``\emph{On some application of statistical
% method}'' from 1916.\ by Markov, A., A., founder of Theory of Markov chains.

Authorship attribution can help in document indexing, document filtering and
hierarchical categorization of web pages \citep{luyckx2005shallow}. These
applications are important information retrieval tasks. It is important to note
that authorship attribution differs from plagiarism detection in that the latter
attempts to detect similarities between two substantially different pieces of
work but is unable to determine if they were produced by the same author
\citep{de2001mining}.

The problem can be divided in three categories \citep{zhao2005effective}: binary,
multi--class and single--class (or one--class) classification. Binary
classification solves the problem of each text from data set being written by one
of two authors. Multi--class classification is generalization of binary
classification in which we have more than two authors. One--class classification
is applied when some of the texts from the data set are written by a particular
author while the authorship of the other texts is unspecified. This
classification ascertains if a given text belong to a single known author or not.

This paper presents a study of multi--class classification for texts in Croatian
language, oriented on evaluation of different text representations. The rest of
the paper is organized as follows. Section 2 discusses related work on authorship
attribution and similar problems. Section 3 describes the used data set. Section
4 introduces methods of text representation we have used. Section 5 describes
classification and Section 6 presents evaluation methods and experiment results.
Conclusion and future work are given in Section 7.

\section{Related work}
Approaches to text representation and classification for authorship attribution
include: stylistic measures \citep{coyotl2006authorship}, syntactic clues
\citep{stamatatos2001computer,uzuner2005comparative}, word-based features
\citep{argamon2005measuring,uzuner2005comparative}, compression algorithms
\citep{kukushkina2001using,zhao2005effective}, grammatical error lookup
\citep{koppel2003exploiting}, language modeling
\citep{peng2003language,coyotl2006authorship}, and vocabulary diversity
\citep{stamatatos2001computer}. Following paragraphs describe these approaches in
more depth and relate our work with the existing research.

\citet{coyotl2006authorship} group methods of authorship attribution into three
main approaches: \emph{stylistic measures}, \emph{syntactic cues} and
\emph{word--based} text features.

\emph{Stylistic measures} based text features are characterized by features that
take into account the length of words and sentences and the richness of the
vocabulary. Various types of style markers can be found in
\citep{luyckx2005shallow}. In \citep{coyotl2006authorship} it is noted that such
features are not sufficient to resolve problems that depend on the genre of the
text and that they lose their meaning when applied over short texts.

When using the \emph{syntactic cues as text features}---informations
related to structure of the language which are obtained by an in--depth syntactic
analysis of texts---a single text is characterized by the presence and
frequency of certain syntactic structures. This characterization is detailed and relevant.
Unfortunately, it is computationally expensive and even impossible to build for
languages lacking the text--processing resources (e.g., POS tagger, syntactic
parser, etc.). It is clearly influenced by the length of texts. Description of
authorship attribution process by using syntactic elements can be found in a very influential work by Stamatos et al.
\citep{stamatatos2001computer}.

Approach that uses \emph{Word--based} text features branches into three
different methods. The first one characterizes texts using a set of
functional words (their presence and frequency), ignoring the content words since
they tend to be highly correlated with the text topics. Second branch
of methods observes a text as a bag--of--words and uses single content words as
text features. This method can be applied only when there is a noticeable
correlation between the authors and the topics. The third branch considers
word \emph{n}-grams as features, i.e., features consisting of sequences of
consecutive words.
% Nevertheless, due to the feature explosion, it tends to use
% only \emph{n}-grams up to three words.

The success of functional words over collocations (certain pairs of words
occurring within a given threshold distance of each other) is shown in
\citep{argamon2005measuring}. However, because of information reduction when 
using functional words as features, one might conclude the opposite -- which would be
wrong. \citet{argamon2005measuring} believe that most of the discriminating power
of collocations is due to the frequent words they contain and not the
collocations themselves.
% They also note that using more training texts than
% features seriously reduces the likelihood of overfitting the model to the
% training data, improving the reliability of results. Similar claim can be found
% in \citep{banko2001scaling} which deals with influence the of corpus size in
% general natural language processing.

A similar comparison of feature types is shown in
\citep{uzuner2005comparative} where the functional words and syntactic
elements are compared in order to identify the author of text. It is
concluded that the syntactic elements of expressions are useful as functional words in solving the
problem.

\citet{kukushkina2001using} explain a method which uses algorithms for data
compression to identify the author. The appendix of this paper shows evaluation results
for authorship attribution with different compression algorithms.
% The idea behind
% the method is to divide texts by authors, compress every set with selected
% algorithm and write the size of the archive. To classify text of an unknown author,
% text is added to each set and than the same data compression algorithm is applied. The author of
% the texts in archive which records the smallest increase in size is declared
% as the author of the new text.
Review of a similar method can be found in 
\citep{zhao2005effective}, where it is noted that the method has obvious omissions. Also, a citation on another work that proves it is given.

\citet{koppel2003exploiting} show the use of grammatical errors and informal
styling (e.g., writing sentences in capital letters) as text features in
order to identify the author. Method is applicable only to unedited texts (blogs,
Internet forums, newsgroups, e--mail messages, etc.).

\citet{peng2003language} suggest the use of \emph{n}-gram language model to identify
the author. A similar method is shown in \citep{coyotl2006authorship}.

\citet{stamatatos2001computer} suggest vocabulary diversity of the authors as
feature for identifying the author. It is measured by the ratio of unique words
and the total size of the text or counting words which occur only once
(\emph{hapax legomena}), or twice (\emph{dis legomena}), etc.
These measures are closely related to length of the text. They also note that
functions defined by Yule (1944) and Honore (1979) should solve this problem,
i.e., they should be constant regarding the length of the text. Analysis of
similar function can be found in the \citep{tweedie1998variable} which claims
that most of these functions are not independent regarding the length of the
text. It is concluded that the vocabulary diversity is an unstable feature for
texts shorter than 1000 words.

Our work is a mix of syntactic clues, word--based features, and stylistic measures, adapted for Croatian language (prosiriti).

\section{Data set}
\label{sec:podatci}
We used an online archive of columns from daily Croatian newspaper ``Jutarnji
list'' available at \url{http://www.jutarnji.hr/komentari/}. The data set
consists of 4571 texts written by 25 authors. The lowest average number of words
in text per author is 315 words, and the highest is 1347 words. An average number
of words in text per author for this data set is 717 words. Number of texts per
author in the set is shown on Figure \ref{fig:articlesPerAuthor}.

Because topics in columns tend to be time--specific and in order to avoid the
learning of topic, the data set is split by dates -- 20\% of newest texts of
each author are taken for testing (hold--out method). Therefore, training data
set contains 3425 texts and testing data set 1146.

\begin{minipage}{0.8\linewidth}
\vspace{10pt}
\centerline{\resizebox{1.4\linewidth}{!}{\input{figures/articlesPerAuthor}}}%
\figcaption{\small \textbf{\textsf{Number of texts per author.}}}%
\label{fig:articlesPerAuthor}
\end{minipage}

\section{Text representation}
When constructing an authorship attribution system, the central issuse is the selection of sufficiently discriminative features. A feature is
discriminative if it is common for one author and rare for all the others. Due to large number of authors complex features are very useful if their
distribution is specific for each author. % (e.g., the frequency distribution of some words).%

%Features can be combined. If complex features are combined, new features can represent more distributions and have greater discriminatory power.%

During the creation of the feature vector, it is neccessary to make the
features independent of the length or of the content of each text. That
dependency reduces generality of system's application and can lead to a decrease of accurancy
(e.g., relating author to concrete topic or terms).

Since features are expressed as real numbers, each text is expressed as a real
vector of appropriate number of dimensions. We combine features simply by creating unions of basic feature sets.

% Depending on selected feature model, text analysis and creation of feature
% vectors can vary from computationally very trivial operations to very complex
% algorithms (such as obtaining syntactic features by analyzing natural language).


\subsection{Functional words frequency ($\mathcal{F}$)}
\label{sec:funkcijske-rijeci}
Functional words are words that have few or none semantic content of their
own, such as adverbs, prepositions, conjunctions, or interjections. They usually
indicate a grammatical relationship or generic property
\citep{zhao2005effective}.

% Usefulness of function words occurrence frequency for authorship attribution lies
% in the fact that they are the indicators of writing style.
Some of the less known functional words, such as prepositions \emph{onkraj},
\emph{namjesto} and \emph{zavrh} are rarely used and may very well suggest the
author. However, even the frequency of frequent functional words can distinguish the author very well. Due to high frequency of use of
functional words and their roles in the grammar, the author usually has no
conscious control over their use in a particular text
\citep{argamon2005measuring}.

Functional words are topic--independent, authors automatically (without conscious
control) use functional words which indicate their own style.
%During the changing
%of the language of documents, it is also neccessary to update the set of
%functional words with which the system works.
It is difficult to predict whether the functional words will give equally good
results for different languages. Despite the size of research on this
topic, due to various languages, the type and the size of the texts, it is
difficult to conclude if these methods are generally effective \citep{zhao2005effective}.

In addition to functional words, auxiliary verbs and pronouns are considered.
Their frequencies might be representative for different author styles.

Building the text feature vector is done by counting the appearances
of functional words, auxiliary verbs and pronouns in the text. The results
are written as vectors where each component corresponds to the number of occurences of
related words divided by the total number of the words in the text. This is done
to remove the dependency on the length of the text. For the functional words that
have not appeared in a text, the resulting component always equals zero.

Label $\mathcal{F}$ denotes evaluation result of this feature in Table
\ref{tbl:eval}.

\subsection{Lexical categories frequency ($\mathcal{C}$)}
\label{sec:rijeci-grupe}
Building of the feature vector is done by counting the number of appearances of
different lexical categories where the categories considered are adverbs,
adpositions, conjunctions, particles, interjections, nouns, verbs, adjectives and
pronouns which are obtained by syntactic analysis of Croatian language. Result is
written as a vector (nine--dimensional vector) and each component of the vector
is divided by the number of words in the text.

\subsection{Idf weighted functional words frequency ($\mathcal{I}$)}
\label{sec:funkcijske-rijeci-idf}
The problem of dependency on the length of the text for the classification using
a SVM (Support Vector Machine) is explained in \citep{diederich2003authorship}.
To avoid the dependency, a combination of $L_p$ normalization of the
length and transformation of terms occurrence frequency such as \emph{idf}
(inverse document frequency) measure was used.

Idf measure is defined as \citep{diederich2003authorship}
\begin{equation}
F_{idf}(t_k) = \log \frac{n_d}{n_d(t_k)},
\label{equ:idf}
\end{equation}
where $n_d(t_k)$ is number of documents which contain term $t_k$ and
$n_d$ total number of documents. The shown measure gives high values for terms
which appear in a small number of documents and thus it is very discriminatory.

The feature vector is built by multiplying components of vector created by method
defined in \ref{sec:funkcijske-rijeci} and its associated \emph{idf} weight.

The shown measure discriminates documents that contain functional words that are
used in small number of other documents. The disadvantage of \emph{idf} measure is
that it records only presence of certain term in the document, term
counting in the document is ignored. Therefore, word which appears many times in
one text and one time in the others gets the same value as the one which appears
once in all text---the word which would very well separate one document from the
others is ignored.

\subsection{Punctation marks ($\mathcal{P}$), vowels ($\mathcal{V}$), words
length ($\mathcal{L}$) and sentence length ($\mathcal{S}$) frequency}
\label{sec:znacajke-manje}
A set of following punctuation marks is used: ``.'', ``,'', ``!'', ``?'',
``''', ``"'', ``-'', ``:'', ``;'', ``+'', ``*'' and their number of appearance in
text is counted. Result is written as 11--dimensional vector and every
component is divided by total sum of characters in the text.

A feature vector based on the frequency of vowel occurence (a, e, i, o, u) is
obtained in equal procedure as for the punctuation marks.

Frequency of word lengths are obtained by counting the words that have equal
length. It is important to note that this procedure can lead to vectors of
different features dimensions (e.g., a text has a word with length of 11,
but some other text does not). The issue is solved by limiting the maximum
length of words at length of 10. All words longer than 10 characters are
counted to the 10th group. It is necessary to divide components of resulting a feature
vector with the number of words in the text in order to diminish the dependency
of features with the text's length.

The sentence length frequency is obtained in equal procedure as for the word
length frequency. An different vectors dimension issue is solved by limiting
sentence length to 20.

Suggested features have weak discriminatory power on their own, but they are
very useful in combination with other features as shown in Section \ref{sec:evaluacija}.

\subsection{Word part--of--speech \emph{n}-grams frequency ($\mathcal{N}_1$ \&
$\mathcal{N}_2$)}
\label{sec:ngrami-tipova}
The two proposed features are based on word part--of--speech \emph{n}-grams
frequency. Word parts--of--speech and their morphosyntactic descriptors are obtained by POS
(Part--Of--Speech) and MSD (morphosyntactic) tagging for Croatian language
\citep{snajder08automatic}. Having the corresponding part--of--speech, for
each word in the text, makes the idea of using \emph{n}-grams as features
possible. As \emph{n}-grams features can produce very large dimensionality 
only 3-grams are considered. In addition POS tagging used is not perfect, the
method does not use context information and therefore cannot distinguish between
different homographs---words with same spelling but with different meaning and probably
different POS too---so all possible POS tags for given word are considered.

First proposed feature uses the words parts--of-speech
(nouns, verbs, adjectives, pronouns, conjunctions, interjections and
prepositions) to form various \emph{n}-grams and count their frequencies. For
example, word 3-gram ``Adam i Eva'' (``Adam and Eve'') forms ``noun
conjunction noun'' trigram. Second proposed feature uses only words
parts--of-speech information for nouns, verbs and adjectives and for other word
parts--of-speech it uses words as they are, therefore ``Adam i Eva'' transforms
to ``noun i noun''. Due to many different pronouns, conjunctions, interjections
and prepositions that make many different \emph{n}-grams, frequency filtering
is applied---only frequency of 500 most frequent 3-grams in
training data set is considered. Used dimension reduction method is not optimal,
therefore in future work other methods should be evaluated, such as information
gain, $\chi^2$ test, mutual information, maximum relevance, minimum redundancy
or classification with sparse SVM, logistic regression or na\"ive Bayes.

\subsection{Word morphologic categories ($\mathcal{M}$)}
\label{sec:morphosyntactic}
Building of feature vector is done by counting appearances of morphologic
categories for every word in text and dividing them by number of words in text.
Counted morphologic categories are \emph{case}, \emph{degree}, \emph{form},
\emph{gender}, \emph{number} and \emph{person}.

\end{multicols}
\begin{table*}[htb]
\begin{center}%
\caption{\small \textbf{\textsf{Evaluation of different features}}}%
\begin{tabular}{r c c r@{.}l}%
\toprule%
Method & Accuracy [\%] & $C$ & \multicolumn{2}{c}{$\gamma$} \\
\midrule
$\mathcal{F}$ & 88.39 & 8192 & 0 & 125\\
$\mathcal{I}$ & 87.96 & 8192 & 0 & 125\\
$\mathcal{C}$ & 44.50 & 512 & 2 & 0\\
$\mathcal{P}$ & 57.50 & 8192 & 0 & 125\\
$\mathcal{V}$ & 30.54 & 128 & 0 & 125\\
$\mathcal{L}$ & 43.19 & 128 & 0 & 125\\
$\mathcal{S}$ & 42.32 & 128 & 0 & 125\\
$\mathcal{N}_1$ & 71.29 & 512 & 0 & 125\\
$\mathcal{N}_2$ & 76.09 & 512 & 0 & 125\\
$\mathcal{M}$ & 61.17 & 512 & 0 & 125\\
$\mathcal{C}$, $\mathcal{M}$ & 63.17 & 8192 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$ & 91.71 & 8 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{M}$ & 91.18 & 128 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{N}_1$ & 89.44 & 128 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{N}_2$ & 88.48 & 128 & 0 & 03125\\
$\mathcal{I}$, $\mathcal{M}$ & 90.84 & 128 & 0 & 03125\\
$\mathcal{N}_1$, $\mathcal{M}$ & 71.38 & 128 & 0 & 03125\\
$\mathcal{I}$, $\mathcal{M}$, $\mathcal{C}$ & 91.36 & 128 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$ & \textbf{93.11} & 128 & 0 & 03125\\
$\mathcal{P}$, $\mathcal{F}$, $\mathcal{L}$, $\mathcal{M}$ & \textbf{93.46} & 32768 & 0 & 03125\\
$\mathcal{F}$, $\mathcal{M}$, $\mathcal{C}$, $\mathcal{N}_1$ & 89.62 & 128 & 0 & 03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$ & \textbf{93.19} & 128 & 0 & 03125\\
$\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$, $\mathcal{L}$, $\mathcal{M}$, $\mathcal{N}_1$ & 92.41 & 128 & 0 & 03125\\
\bottomrule%
\end{tabular}%
\label{tbl:eval}%
\end{center}
\end{table*}
\begin{multicols}{2}

\section{Classification}
% Representation of documents by real number vectors  enables easy use of
% classifiers that search decision functions, i.e., boundaries in vector
% space.
The classifier used is SVM (Support Vector Machine) with radial basis
function as the kernel. It is shown that, with the use of parameter selection,
linear SVM is special case of SVM with RBF kernel \citep{keerthi2003asymptotic}
what removes the need to consider linear SVM as potential classifier.

Before the use of SVM, it is required to scale the data to ensure
equal contribution of every attribute to the classification. Components of
every feature vector are scaled to interval $[0, 1]$.
%  according to following expression:
% \begin{equation}
% x^{s}_{i,j} = \frac{x_{i,j} - \min_{i}\; x_{i,j}}{\max_{i}\; x_{i,j}
% - \min_{i}\; x_{i,j}}
% \end{equation}
% where $x^{s}_{i,j}$ is scaled component $j$ of vector $\mathbf{x_i}$,
% $\min_{i}\; x_{i,j}$ is minimum value of attribute $j$ among all vectors
% $\mathbf{x_i}$ and $\max_{i}\; x_{i,j}$ is maximum value of attribute $j$ among
% all vectors. If we denote the resulting minimum and maximum values as follows:
% \begin{eqnarray}
% M_i & = \max_{i}\; x_{i,j} \\
% m_i & = \min_{i}\; x_{i,j}
% \end{eqnarray}
% then, the unknown vector $\mathbf{x}$ before classification is scaled as:
% \begin{equation}
% x^{s}_{j} = \frac{x_j-m_i}{M_i-m_i}
% \end{equation}

% TODO: REPHRASE
Finding appropriate paramters $(C, \gamma)$ is done by the means of
cross-validation: using the 5-fold cross--validation on the
learning set parameters $(C, \gamma)$ which give the highest accuracy are
selected. Parameters that were considered are: $C \in \{2^{-5}, 2^{-4}, \cdots ,
2^{15}\}$, $\gamma \in \{2^{-15}, 2^{-14}, \cdots, 2^3\}$ \citep{CC01a}.
The accuracy of classification is measured by the expression:
\begin{equation}
acc = \frac{n_c}{N}, % ako bude problema, ovo staviti kao ``inline'' jednadžbu.
\end{equation}
where $n_c$ is number of correctly classified texts, $N$ is total number of
texts. After the best parameters  $(C, \gamma)$ are found for
which the system achives the highest accuracy, the classifier is learned using
those parameters.

\section{Evaluation}
\label{sec:evaluacija}
Classification success is measured by ratio of correctly classified texts and
total number of texts (accuracy). Results of evalutaion are shown in Table
\ref{tbl:eval}. Columns ``$C$'' and ``$\gamma$'' denote parameters of SVM
classifier used for selected model.

% Highest accuracy during cross--validation is achived by combination of
% methods marked as $\mathcal{S}$, $\mathcal{P}$, $\mathcal{F}$, $\mathcal{V}$,
% $\mathcal{L}$ and $\mathcal{M}$.

%% TODO: Što s ovim?
% Total accuracy doesn't explains behaviour of classifier for every class by
% itself. For every class precision and recall are calculated and by them we
% calculate total weighted $F$ measure. For particular class $c$, precision is
% ratio of number of correctly classified documents in $c$ with number of all
% documents which are classified as $c$. Recall is ratio of number of correctly
% classified documents in $c$ with number of all documents in $c$. $F$ measure is
% calculated for every class $c_i$ according to following expression:
% \begin{equation}
% F_i = \frac{2 \cdot precision_i \cdot recall_i}{precision_i + recall_i}
% \end{equation}
% where $precision_i$ and $recall_i$ are measures of precision and recall for
% class $c_i$.
% 
% Total weighted measure is calculated by following expression:
% \begin{equation}
% F_u = \frac{\sum^{n}_i |c_i|\cdot F_i}{\sum^n_i|c_i|}
% \end{equation}
% where $n$ is total number of classes, $|c_i|$ number of documents in class
% $c_i$ and $F_i$ a $F$ measure for class $c_i$.
% 
% Weighted $F$ measure at our data set for testing gives value of 87\%.

\section{Conclusion}
It is shown that authorship attribution problem can be successfully solved by
relatively simple methods, although for state--of--the--art methods, we
believe that syntactic analysis of texts is required. Our results with
93\% accuracy fit very well in interval of reported results which range from
70\% to 97\% \citep{coyotl2006authorship,keselj2003n,luyckx2005shallow,stamatatos2001computer}.

Result comparision is difficult due to different types of data sets (e.g., poems,
newspaper articles, e--mails) and the problems (binary, multi--class or
single--class classifications). There are no relevant data sets for comparision
\citep{zhao2005effective}, but there are references on paper
\citep{stamatatos2001computer} and their ``Greek data set'' (e.g.,
\citep{keselj2003n}). There is an significant affected of data set type (number
and variety of text samples) on the complexity of the problem.

In future work, methods based on word and character \emph{n}-grams suggested
in \citep{keselj2003n,peng2003language,coyotl2006authorship} has to be
evaluated. Also, evaluation has to be performed on different types of data sets
such as poems, newspaper articles or books data sets.

\section*{Acknowledgement}
% TODO: Napisati

\bibliographystyle{plainnat}
\bibliography{literatura}

\end{multicols}

\end{document}
